{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import configparser\n",
    "from functools import partial\n",
    "import time\n",
    "import numpy as np\n",
    "from shapely import Polygon\n",
    "import math\n",
    "from typing import Any\n",
    "from matplotlib.backend_bases import MouseEvent, MouseButton\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "from bbtoolkit.utils.viz import plot_arrow, plot_polygon\n",
    "from bbtoolkit.utils.datautils import Cached\n",
    "from bbtoolkit.utils.datautils.configparser import EvalConfigParser\n",
    "from bbtoolkit.environment import Environment\n",
    "from bbtoolkit.environment.compilers import DynamicEnvironmentCompiler\n",
    "from bbtoolkit.environment.compilers.callbacks import TransparentObjects\n",
    "from bbtoolkit.environment.utils import env2builder\n",
    "from bbtoolkit.environment.visible_planes import LazyVisiblePlaneWithTransparancy\n",
    "from bbtoolkit.models.bb.neural_generators import TCGenerator\n",
    "from bbtoolkit.structures.geometry import Texture, TexturedPolygon\n",
    "from bbtoolkit.dynamics.callbacks import BaseCallback\n",
    "from bbtoolkit.environment.fov import FOVManager\n",
    "from bbtoolkit.environment.fov.ego import EgoManager\n",
    "from bbtoolkit.utils.math import pol2cart\n",
    "from bbtoolkit.utils.math.geometry import calculate_polar_distance\n",
    "from bbtoolkit.dynamics import DynamicsManager\n",
    "from bbtoolkit.dynamics.callbacks.fov import EgoCallback, EgoSegmentationCallback, FOVCallback, ParietalWindowCallback\n",
    "from bbtoolkit.dynamics.callbacks.movement import MovementCallback, MovementSchedulerCallback, TrajectoryCallback\n",
    "from bbtoolkit.utils.movement import MovementManager\n",
    "from bbtoolkit.utils.attention import RhythmicAttention\n",
    "from bbtoolkit.dynamics.callbacks.attention import AttentionCallback\n",
    "from bbtoolkit.utils.movement.trajectory import AStarTrajectory\n",
    "from bbtoolkit.structures.tensorgroups import DirectedTensor\n",
    "from bbtoolkit.models.bb.neural_generators import MTLGenerator\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Transformation Circuit and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hd_config_path = '../cfg/cells/hd_cells.ini'\n",
    "hd_config = EvalConfigParser(interpolation=configparser.ExtendedInterpolation(), allow_no_value=True)\n",
    "hd_config.read(hd_config_path)\n",
    "\n",
    "mtl_config_path = '../cfg/cells/mtl_cells.ini'\n",
    "mtl_config = EvalConfigParser(interpolation=configparser.ExtendedInterpolation(), allow_no_value=True)\n",
    "mtl_config.read(mtl_config_path)\n",
    "\n",
    "tr_config_path = '../cfg/cells/transformation_circuit.ini'\n",
    "tr_config = EvalConfigParser(interpolation=configparser.ExtendedInterpolation(), allow_no_value=True)\n",
    "tr_config.read(tr_config_path)\n",
    "\n",
    "env_cfg = EvalConfigParser(interpolation=configparser.ExtendedInterpolation(), allow_no_value=True)\n",
    "env_cfg.read('../cfg/envs/squared_room.ini')\n",
    "\n",
    "space_cfg = mtl_config['Space']\n",
    "h_res = space_cfg.eval('res')\n",
    "r_max = space_cfg.eval('r_max')\n",
    "\n",
    "mtl_grid_cfg = mtl_config['PolarGrid']\n",
    "n_radial_points = mtl_grid_cfg.eval('n_radial_points')\n",
    "polar_dist_res = mtl_grid_cfg.eval('polar_dist_res')\n",
    "polar_ang_res = mtl_grid_cfg.eval('polar_ang_res', locals={'pi': np.pi})\n",
    "h_sig = mtl_grid_cfg.eval('sigma_hill')\n",
    "\n",
    "tr_space_cfg = tr_config['Space']\n",
    "tr_res = tr_space_cfg.eval('tr_res', locals={'pi': np.pi})\n",
    "res = tr_space_cfg.eval('res')\n",
    "\n",
    "n_steps = tr_config['Training'].eval('n_steps')\n",
    "\n",
    "hd_neurons_cfg = hd_config['Neurons']\n",
    "sigma_angular = hd_neurons_cfg.eval('sigma', locals={'pi': np.pi})\n",
    "n_hd = hd_neurons_cfg.eval('n_neurons')\n",
    "\n",
    "\n",
    "training_rect_cfg = env_cfg['TrainingRectangle']\n",
    "max_train_x = training_rect_cfg.eval('max_train_x')\n",
    "min_train_x = training_rect_cfg.eval('min_train_x')\n",
    "max_train_y = training_rect_cfg.eval('max_train_y')\n",
    "min_train_y = training_rect_cfg.eval('min_train_y')\n",
    "\n",
    "# env = Environment.load('../data/envs/main_environment.pkl')\n",
    "env = Environment.load('../data/envs/square_environment.pkl')\n",
    "\n",
    "tc_gen = TCGenerator(\n",
    "    n_hd,\n",
    "    tr_res,\n",
    "    res,\n",
    "    r_max,\n",
    "    polar_dist_res,\n",
    "    n_radial_points,\n",
    "    polar_ang_res,\n",
    "    sigma_angular\n",
    ")\n",
    "\n",
    "builder = env2builder(env)\n",
    "cache_manager = Cached(cache_storage=OrderedDict(), max_size=10000)\n",
    "compiler = DynamicEnvironmentCompiler(\n",
    "    builder,\n",
    "    partial(\n",
    "        LazyVisiblePlaneWithTransparancy,\n",
    "        cache_manager=cache_manager,\n",
    "    ),\n",
    "    callbacks=[TransparentObjects()]\n",
    ")\n",
    "\n",
    "compiler.add_object(\n",
    "    TexturedPolygon(\n",
    "        Polygon([\n",
    "            (-5, -5),\n",
    "            (-6, -5),\n",
    "            (-6, -6),\n",
    "            (-5, -6)\n",
    "        ]),\n",
    "        texture=Texture(\n",
    "            id_=31,\n",
    "            color='#ffd200',\n",
    "            name='main_object'\n",
    "        )\n",
    "    ),\n",
    "    TexturedPolygon(\n",
    "        Polygon([\n",
    "            (-7, -7),\n",
    "            (-8, -7),\n",
    "            (-8, -8),\n",
    "            (-7, -8)\n",
    "        ]),\n",
    "        texture=Texture(\n",
    "            id_=32,\n",
    "            color='#ffd200',\n",
    "            name='main_object'\n",
    "        )\n",
    "    ),\n",
    "    TexturedPolygon(\n",
    "        Polygon([\n",
    "            (2, 2),\n",
    "            (1, 2),\n",
    "            (1, 1),\n",
    "            (2, 1)\n",
    "        ]),\n",
    "        texture=Texture(\n",
    "            id_=33,\n",
    "            color='#ffd200',\n",
    "            name='main_object'\n",
    "        )\n",
    "    ),\n",
    "    TexturedPolygon(\n",
    "        Polygon([\n",
    "            (-2, 2),\n",
    "            (-1, 2),\n",
    "            (-1, 1),\n",
    "            (-2, 1)\n",
    "        ]),\n",
    "        texture=Texture(\n",
    "            id_=34,\n",
    "            color='#ffd200',\n",
    "            name='main_object'\n",
    "        )\n",
    "    ),\n",
    "    TexturedPolygon(\n",
    "        Polygon([\n",
    "            (7, 7),\n",
    "            (6, 7),\n",
    "            (6, 6),\n",
    "            (7, 6)\n",
    "        ]),\n",
    "        texture=Texture(\n",
    "            id_=35,\n",
    "            color='#ffd200',\n",
    "            name='main_object'\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbtoolkit.structures.tensorgroups import DirectedTensorGroup, dict2directed_tensor\n",
    "\n",
    "def connectivity_config2dict(\n",
    "    config: EvalConfigParser,\n",
    "    populations: tuple[str, ...] = None,\n",
    "    ignore: tuple[str, ...] = None\n",
    ") -> dict[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Converts EvalConfigParser object into a nested dictionary with specified sections and populations.\n",
    "\n",
    "    Args:\n",
    "        config (EvalConfigParser): An instance of EvalConfigParser containing configuration data.\n",
    "        populations (tuple[str, ..], optional): A tuple of sections to include.\n",
    "            If None, read all sections except of 'ExternalSources' and 'Hyperparameters'. Defaults to None.\n",
    "        ignore (tuple[str, ..], optional): A tuple of sections to ignore. 'ExternalSources' and 'Hyperparameters' sections are always ignored. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, dict[str, Any]]: A nested dictionary containing configuration data organized by sections and populations.\n",
    "    \"\"\"\n",
    "    if populations is None:\n",
    "        populations = tuple(config.sections())\n",
    "\n",
    "    if ignore is None:\n",
    "        ignore = {'ExternalSources', 'Hyperparameters'}\n",
    "    else:\n",
    "        ignore = set(list(ignore) + ['ExternalSources', 'Hyperparameters'])\n",
    "\n",
    "    out = {\n",
    "        config.optionxform(section): {\n",
    "            population: config.eval(section, population)\n",
    "            for population in config[section]\n",
    "        }\n",
    "        for section in config.sections() if section in populations and section not in ignore\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "config = EvalConfigParser(interpolation=configparser.ExtendedInterpolation(), allow_no_value=True)\n",
    "config.read('../cfg/connectivity/main.ini')\n",
    "\n",
    "configdict = connectivity_config2dict(config)\n",
    "connections = dict2directed_tensor(configdict)\n",
    "connectivity = DirectedTensorGroup(*connections)\n",
    "\n",
    "position = 1, -5\n",
    "# direction = -np.pi/2\n",
    "direction = 0\n",
    "fov_angle = np.pi*.9\n",
    "fov_manager = FOVManager(compiler.environment, fov_angle)\n",
    "ego_manager = EgoManager(fov_manager)\n",
    "\n",
    "cache = {'env': compiler.environment, 'tc_gen': tc_gen}\n",
    "dt = 0.01\n",
    "n_objects = 5\n",
    "\n",
    "polar_distance = calculate_polar_distance(tc_gen.r_max)\n",
    "polar_angle = np.linspace(0, (tc_gen.n_bvc_theta + 1) * tc_gen.polar_ang_res, tc_gen.n_bvc_theta)\n",
    "polar_distance, polar_angle = np.meshgrid(polar_distance, polar_angle)\n",
    "pdist, pang = polar_distance, polar_angle\n",
    "x_bvc, y_bvc = pol2cart(pdist, pang)\n",
    "hd_polar_res = 2 * np.pi / n_hd\n",
    "hd_angles = np.arange(0, 2 * np.pi+ hd_polar_res, hd_polar_res) + np.pi/2\n",
    "hd_dist, hd_ang = np.meshgrid(np.array([1, 1.5]), hd_angles)\n",
    "hd_x, hd_y = pol2cart(hd_dist, hd_ang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbtoolkit.models.bb.neural_generators import GCMap\n",
    "\n",
    "# trunc means not including boundaries themselves\n",
    "# mtl_weights = DirectedTensorGroup.load('../data/weights/mtl_weights_trunc_square.pkl')\n",
    "# mtl_weights = DirectedTensorGroup.load('../data/weights/mtl_weights_square_check.pkl')\n",
    "mtl_weights = DirectedTensorGroup.load('../data/weights/mtl_weights_square_check4.pkl')\n",
    "tc_weights = DirectedTensorGroup.load('../data/weights/tc_weights.pkl')\n",
    "hd_weights = DirectedTensorGroup.load('../data/weights/hd_weights.pkl')\n",
    "# pc_weights = DirectedTensorGroup.load('../data/weights/pc_weights_trunc.pkl')\n",
    "pc_weights = DirectedTensorGroup.load('../data/weights/pc_weights.pkl')\n",
    "gc_map = GCMap.load('../data/weights/gc_map.pkl')\n",
    "\n",
    "weights = mtl_weights + tc_weights + hd_weights + pc_weights\n",
    "\n",
    "n_h_neurons = len(weights.h.to.h)\n",
    "n_bvc_neurons = n_opw_neurons = n_ovc_neurons = len(weights.bvc.to.bvc)\n",
    "n_pr_neurons = len(weights.pr.to.pr)\n",
    "n_hd_neurons = len(weights.hd.to.hd)\n",
    "n_pc_neurons = len(weights.gc.to.h)\n",
    "n_gc_neurons = len(weights.gc.to.h.T)\n",
    "n_tc_layers = len(weights.tr.to.pw.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbtoolkit.structures.tensorgroups import DirectedTensor\n",
    "\n",
    "\n",
    "tr_projection = DirectedTensorGroup(\n",
    "    DirectedTensor(\n",
    "        from_='bvc',\n",
    "        to='tr',\n",
    "        weights=np.eye(n_bvc_neurons)\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='tr',\n",
    "        to='bvc',\n",
    "        weights=np.eye(n_bvc_neurons)\n",
    "    )\n",
    ")\n",
    "\n",
    "new_pathways = DirectedTensorGroup(\n",
    "    DirectedTensor(\n",
    "        from_='h',\n",
    "        to='opr',\n",
    "        weights=np.zeros((n_objects, n_h_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='opr',\n",
    "        to='h',\n",
    "        weights=np.zeros((n_h_neurons, n_objects))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='h',\n",
    "        to='ovc',\n",
    "        weights=np.zeros((n_ovc_neurons, n_h_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='ovc',\n",
    "        to='h',\n",
    "        weights=np.zeros((n_h_neurons, n_ovc_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='ovc',\n",
    "        to='ovc',\n",
    "        weights=np.zeros((n_ovc_neurons, n_ovc_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='bvc',\n",
    "        to='ovc',\n",
    "        weights=np.zeros((n_ovc_neurons, n_bvc_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='ovc',\n",
    "        to='bvc',\n",
    "        weights=np.zeros((n_bvc_neurons, n_ovc_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='opw',\n",
    "        to='opw',\n",
    "        weights=np.zeros((n_opw_neurons, n_opw_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='ovc',\n",
    "        to='opr',\n",
    "        weights=np.zeros((n_objects, n_ovc_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='opr',\n",
    "        to='ovc',\n",
    "        weights=np.zeros((n_ovc_neurons, n_objects))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='opr',\n",
    "        to='pw',\n",
    "        weights=np.zeros((n_opw_neurons, n_objects))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='opr',\n",
    "        to='hd',\n",
    "        weights=np.zeros((n_hd_neurons, n_objects))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='bvc',\n",
    "        to='opr',\n",
    "        weights=np.zeros((n_objects, n_bvc_neurons))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='opr',\n",
    "        to='bvc',\n",
    "        weights=np.zeros((n_bvc_neurons, n_objects))\n",
    "    ),\n",
    "    DirectedTensor(\n",
    "        from_='opr',\n",
    "        to='opr',\n",
    "        weights=np.zeros((n_objects, n_objects))\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights += tr_projection\n",
    "weights += new_pathways\n",
    "\n",
    "weights = weights.operation_with(connectivity, lambda a, b: a - b.get('inhibitory_phi', 0), on_missing_weights='ignore', on_missing_sources='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbtoolkit.utils.math.geometry import points2segments\n",
    "from bbtoolkit.structures.tensorgroups import NamedTensor, TensorGroup\n",
    "\n",
    "\n",
    "activity = TensorGroup(\n",
    "    NamedTensor('h', np.zeros((n_h_neurons, 1))),\n",
    "    NamedTensor('bvc', np.zeros((n_bvc_neurons, 1))),\n",
    "    NamedTensor('pr', np.zeros((n_pr_neurons, 1))),\n",
    "    NamedTensor('pw', np.zeros((n_bvc_neurons, 1))),\n",
    "    NamedTensor('hd', np.zeros((n_hd_neurons, 1))),\n",
    "    NamedTensor('ovc', np.zeros((n_ovc_neurons, 1))),\n",
    "    NamedTensor('opw', np.zeros((n_opw_neurons, 1))),\n",
    "    NamedTensor('opr', np.zeros((n_objects, 1))),\n",
    "    NamedTensor('tr', np.zeros((n_tc_layers, n_bvc_neurons, 1))),\n",
    "    NamedTensor('otr', np.zeros((n_tc_layers, n_bvc_neurons, 1)))\n",
    ")\n",
    "\n",
    "walls_ego, objects_ego = ego_manager(position, direction)\n",
    "wall_segments = np.concatenate([points2segments(wall) for wall in walls_ego if wall.size])\n",
    "ego_input_walls = tc_gen.get_grid_activity(wall_segments)\n",
    "hd_cue_init = 40*tc_gen.get_hd_activity(np.array([direction]))\n",
    "hd_cue = np.zeros_like(hd_cue_init)\n",
    "ohd_cue = hd_cue.copy()\n",
    "pr_cue = np.zeros((n_pr_neurons, 1))\n",
    "\n",
    "k_ratio = TensorGroup(\n",
    "    NamedTensor('h', np.zeros((n_h_neurons, 1))),\n",
    "    NamedTensor('bvc', np.zeros((n_bvc_neurons, 1))),\n",
    "    NamedTensor('pr', np.zeros((n_pr_neurons, 1))),\n",
    "    NamedTensor('opr', np.zeros((n_objects, 1))),\n",
    "    NamedTensor('hd', np.zeros((n_hd_neurons, 1))),\n",
    "    NamedTensor('pw', np.expand_dims(40*ego_input_walls, 1)), # FIXME: why 40?\n",
    "    NamedTensor('ovc', np.zeros((n_ovc_neurons, 1))),\n",
    "    NamedTensor('opw', np.zeros((n_opw_neurons, 1))),\n",
    "    NamedTensor('tr', np.zeros((n_tc_layers, n_bvc_neurons, 1))),\n",
    "    NamedTensor('otr', np.zeros((n_tc_layers, n_bvc_neurons, 1))),\n",
    ")\n",
    "\n",
    "activity.pw += dt/connectivity.pw.to.pw['tau']*k_ratio.pw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity2rate(activity: TensorGroup, connectivity: DirectedTensorGroup) -> TensorGroup:\n",
    "    \"\"\"\n",
    "    Converts the activity of a TensorGroup to firing rates using a sigmoid projection.\n",
    "\n",
    "    Args:\n",
    "        activity (TensorGroup): The activity of the neurons.\n",
    "        connectivity (DirectedTensorGroup): The synaptic connectivity between the neurons.\n",
    "\n",
    "    Returns:\n",
    "        TensorGroup: The firing rates of the neurons.\n",
    "    \"\"\"\n",
    "    rates = TensorGroup()\n",
    "    for key, tensor in activity.data.items():\n",
    "        key_ = 'pr' if 'pr' in key else key # object identity cells, use same beta and alpha as other PR neurons by definition\n",
    "        rates.add_tensor(\n",
    "            NamedTensor(\n",
    "                key,\n",
    "                1/(\n",
    "                    1 + np.exp(-2*connectivity[key_, key_]['beta']*(tensor - connectivity[key_, key_]['alpha']))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    return rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = activity2rate(activity, connectivity)\n",
    "rates.add_tensor(NamedTensor('gc', np.array([])))\n",
    "rates.add_tensor(NamedTensor('ip', np.array([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "\n",
    "class HDCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed for handling head direction (HD) cues and updating HD cell activities in an agent-based learning simulation.\n",
    "\n",
    "    This callback integrates external HD cues, manages HD cell activities based on movement and cognitive states, and updates the agent's perceived direction.\n",
    "\n",
    "    Attributes:\n",
    "        init_timesteps (int): The number of initial timesteps during which HD cues are applied.\n",
    "        hd_cue_scale (float): The scale factor for HD cues.\n",
    "        no_cue_reset_modes (tuple[str, ...]): Modes in which HD cues are not reset.\n",
    "        total_steps (int): Counter for the steps during which HD cues are active.\n",
    "        mode (str): The current mode of the simulation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_timesteps: int = 30,\n",
    "        hd_cue_scale: float = 60,\n",
    "        no_cue_reset_modes: tuple[str, ...] = ('recall', 'top-down')\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the HDCallback instance with specified parameters for HD cue initialization and scaling.\n",
    "\n",
    "        Args:\n",
    "            init_timesteps (int): The number of initial timesteps during which HD cues are applied.\n",
    "            hd_cue_scale (float): The scale factor for HD cues.\n",
    "            no_cue_reset_modes (tuple[str, ...]): Modes in which HD cues are not reset.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.init_timesteps = init_timesteps\n",
    "        self.hd_cue_scale = hd_cue_scale\n",
    "        self.total_steps = None\n",
    "        self.mode = None\n",
    "        self.no_cue_reset_modes = no_cue_reset_modes\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for HD cue management, and initializes HD cues.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `tc_gen`: Transformation circuit generator for HD activity.\n",
    "            - `movement_params`: Contains the physical movement parameters of the agent, including its current position.\n",
    "            - `mental_movement_params`: Contains the mental movement parameters of the agent during cognitive navigation tasks.\n",
    "            - `hd_cue`: HD cue array for external directional cues.\n",
    "            - `k_ratio`: Ratio of excitation/inhibition for HD cells.\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `connectivity`: Connectivity matrices between neural populations.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',  # Dynamic parameters including dt, mode, and step.\n",
    "            'tc_gen',  # Transformation circuit generator for HD activity.\n",
    "            'movement_params',  # Movement parameters including position and direction.\n",
    "            'mental_movement_params',  # Mental movement parameters for cognitive navigation.\n",
    "            'hd_cue',  # HD cue array for external directional cues.\n",
    "            'k_ratio',  # Ratio of excitation/inhibition for HD cells.\n",
    "            'activity',  # Neural activity levels.\n",
    "            'connectivity',  # Connectivity matrices between neural populations.\n",
    "            'weights',  # Synaptic weights between neurons.\n",
    "            'rates'  # Firing rates of neurons.\n",
    "        ]\n",
    "\n",
    "        cache['hd_cue'] = np.zeros(len(cache['weights'].hd.to.hd))\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates HD cues and cell activities at the beginning of each simulation step, based on the agent's movement and cognitive state.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if self.mode != self.dynamics_params['mode']:\n",
    "            if self.dynamics_params['mode'] not in self.no_cue_reset_modes:\n",
    "                self.total_steps = self.init_timesteps\n",
    "\n",
    "            self.mode = self.dynamics_params['mode']\n",
    "\n",
    "        if self.total_steps == self.init_timesteps:\n",
    "            logging.debug('HD CUE INITIATED')\n",
    "\n",
    "\n",
    "        if self.total_steps != 0:\n",
    "            self.total_steps -= 1\n",
    "            self.hd_cue += self.hd_cue_scale*self.tc_gen.get_hd_activity(np.array([self.movement_params.direction]))\n",
    "        else:\n",
    "            if not np.all(self.hd_cue == 0):\n",
    "                logging.debug('HD CUE REMOVED')\n",
    "                self.hd_cue *= 0\n",
    "\n",
    "        rot_weights = None\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                params = self.movement_params\n",
    "                target = self.movement_params.move_target if self.movement_params.move_target is not None else self.movement_params.rotate_target\n",
    "                position = self.movement_params.position\n",
    "                direction = self.movement_params.direction\n",
    "            case 'top-down':\n",
    "                params = self.mental_movement_params\n",
    "                target = self.mental_movement_params.move_target if self.mental_movement_params.move_target is not None else self.mental_movement_params.rotate_target\n",
    "                position = self.mental_movement_params.position\n",
    "                direction = self.mental_movement_params.direction\n",
    "            case 'recall':\n",
    "                params, target, position, direction = None, None, None, None\n",
    "\n",
    "        if target is not None:\n",
    "            angle_to_target = math.atan2(\n",
    "                target[1] - position[1],\n",
    "                target[0] - position[0]\n",
    "            ) % (2*np.pi)\n",
    "\n",
    "            diff = angle_to_target - direction\n",
    "            if diff > np.pi:\n",
    "                diff -= 2*np.pi\n",
    "            elif diff < -np.pi:\n",
    "                diff += 2*np.pi\n",
    "\n",
    "            if diff < 0:\n",
    "                rot_weights = self.weights.rot.to.rot.T\n",
    "            elif diff > 0:\n",
    "                rot_weights = self.weights.rot.to.rot\n",
    "            elif np.isclose(angle_to_target, direction):\n",
    "                rot_weights = None\n",
    "\n",
    "        if rot_weights is None:\n",
    "            rot_weights = np.zeros_like(self.weights.rot.to.rot)\n",
    "\n",
    "        self.k_ratio.hd = -self.activity.hd +\\\n",
    "            (self.connectivity.hd.to.hd['phi']*self.weights.hd.to.hd@self.rates.hd) +\\\n",
    "            self.hd_cue[:, np.newaxis] +\\\n",
    "            (self.connectivity.rot.to.rot['phi']*rot_weights@self.rates.hd)\n",
    "\n",
    "        if self.dynamics_params['mode'] in ('recall', 'top-down'):\n",
    "            self.k_ratio.hd += self.connectivity.opr.to.hd['phi'] * self.weights.opr.to.hd@self.rates.opr\n",
    "\n",
    "        self.activity.hd += self.dt/self.connectivity.hd.to.hd['tau']*self.k_ratio.hd\n",
    "        self.rates.hd = 1/(1 + np.exp(-2*self.connectivity.hd.to.hd['beta']*(self.activity.hd - self.connectivity.hd.to.hd['alpha'])))\n",
    "\n",
    "        # HD estimation\n",
    "        if self.total_steps == 0 and self.dynamics_params['mode'] != 'recall':\n",
    "            popmax = np.where(self.rates.hd == np.max(self.rates.hd))[0][0]\n",
    "            hd_estim = popmax*2*np.pi/(len(self.rates.hd) - 1) % (2*np.pi)\n",
    "\n",
    "            params.direction = hd_estim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCRateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update grid cell (GC) activation rates based on the agent's current or mental position in an agent-based learning simulation.\n",
    "\n",
    "    This callback uses the agent's physical position in bottom-up mode and the mental position in top-down or recall modes to determine the corresponding grid cell activations.\n",
    "\n",
    "    Attributes:\n",
    "        gc_map (GCMap): An instance of GCMap containing the firing rates of grid cells across different locations.\n",
    "    \"\"\"\n",
    "    def __init__(self, gc_map: GCMap):\n",
    "        \"\"\"\n",
    "        Initializes the GCRateCallback instance with a specified GCMap.\n",
    "\n",
    "        Args:\n",
    "            gc_map (GCMap): An instance of GCMap to be used for updating grid cell activation rates.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gc_map = gc_map.fr\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating GC rates, and prepares for rate updates.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `movement_params`: Contains the physical movement parameters of the agent, including its current position.\n",
    "            - `mental_movement_params`: Contains the mental movement parameters of the agent during cognitive navigation tasks.\n",
    "            - `rates`: A structure to store the updated grid cell activation rates.\n",
    "            - `grid2cart`: A mapping function or structure to convert grid indices to Cartesian coordinates.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'movement_params',\n",
    "            'mental_movement_params',\n",
    "            'rates',\n",
    "            'grid2cart'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def get_grid_location(self, x: float, y: float) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Converts Cartesian coordinates to grid indices.\n",
    "\n",
    "        Args:\n",
    "            x (float): The x-coordinate.\n",
    "            y (float): The y-coordinate.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: The corresponding grid indices.\n",
    "        \"\"\"\n",
    "        return self.cache.grid2cart(x, y)\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the grid cell activation rates at the beginning of each simulation step, based on the agent's current or mental position.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        match self.dynamics_params.mode:\n",
    "            case 'bottom-up':\n",
    "                if self.movement_params.position is not None:\n",
    "                    self.rates.gc = np.reshape(self.gc_map[*self.get_grid_location(*self.movement_params.position)], (-1, 1))\n",
    "            case 'top-down' | 'recall':\n",
    "                if self.mental_movement_params.position is not None:\n",
    "                    self.rates.gc = np.reshape(self.gc_map[*self.get_grid_location(*self.mental_movement_params.position)], (-1, 1))\n",
    "\n",
    "\n",
    "class PCCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update place cell (PC) dynamics in an agent-based learning simulation.\n",
    "\n",
    "    This callback computes the activity and rates of place cells based on various inputs and connectivity parameters, adjusting for intrinsic competition among cells.\n",
    "\n",
    "    Attributes:\n",
    "        i_comp (float): The initial value for inhibitory compencation.\n",
    "        i_comp_scale (float): The addictive scaling factor for inhibitory compencation adjustment.\n",
    "    \"\"\"\n",
    "    def __init__(self, i_comp: float = 0, i_comp_scale: float = 15):\n",
    "        \"\"\"\n",
    "        Initializes the PCCallback instance with specified parameters for intrinsic competition.\n",
    "\n",
    "        Args:\n",
    "            i_comp (float): The initial value for inhibitory compencation.\n",
    "            i_comp_scale (float): The scaling factor for inhibitory compencation adjustment.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.i_comp = i_comp\n",
    "        self.i_comp_scale = i_comp_scale\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, adds a tensor for intrinsic competition, and specifies the required cache keys for PC dynamics update.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `k_ratio`: Ratio of excitation/inhibition for PC cells.\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `connectivity`: Connectivity matrices between neural populations.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "            - `grid2cart`: A mapping function or structure to convert grid indices to Cartesian coordinates.\n",
    "        \"\"\"\n",
    "        cache['rates'].add_tensor(NamedTensor('i_comp', np.array([self.i_comp]).astype(float)))\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates',\n",
    "            'grid2cart'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the activity and rates of place cells at the beginning of each simulation step, based on the current mode of operation and intrinsic competition among cells.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.cache['k_ratio'].h = (\n",
    "            - self.activity.h\n",
    "            + self.connectivity.h.to.h['phi']*self.weights.h.to.h@self.rates.h\n",
    "            + self.connectivity.pr.to.h['phi']*self.weights.pr.to.h@self.rates.pr\n",
    "            + self.connectivity.ovc.to.h['phi']*self.weights.ovc.to.h@self.rates.ovc\n",
    "            + self.rates.i_comp\n",
    "        )\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                self.cache['k_ratio'].h += self.connectivity.bvc.to.h['phi']*self.weights.bvc.to.h@self.rates.bvc\n",
    "            case 'recall':\n",
    "                self.cache['k_ratio'].h += self.connectivity.opr.to.h['phi']*self.weights.opr.to.h@self.rates.opr\n",
    "            case 'top-down':\n",
    "                self.cache['k_ratio'].h += self.connectivity.opr.to.h['phi']*self.weights.opr.to.h@self.rates.opr +\\\n",
    "                    self.connectivity.gc.to.h['phi']*self.weights.gc.to.h@self.rates.gc\n",
    "\n",
    "        self.activity.h += self.dt/self.connectivity.h.to.h['tau']*self.cache['k_ratio'].h\n",
    "        self.rates.h = 1/(1 + np.exp(-2*self.connectivity.h.to.h['beta']*(self.activity.h - self.connectivity.h.to.h['alpha'])))\n",
    "        # FIXME: What is the 15 in the equation?\n",
    "        self.rates.i_comp += self.dt/self.connectivity.ic.to.ic['tau']*(self.i_comp_scale - np.sum(self.rates.h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BVCCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update the Border Vector Cell (BVC) activity and rates based on the current dynamics of the simulation.\n",
    "\n",
    "    This callback calculates the BVC activity and rates by considering inputs from various sources, including other BVCs, Object Vector Cells (OVCs), Place Cells (PCs), and possibly others depending on the simulation mode.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to compute the BVC activity and rates, including dynamics parameters, connectivity, and weights.\n",
    "    \"\"\"\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating BVC activity and rates, and prepares for computation.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `k_ratio`: Ratio of excitation/inhibition for BVC cells.\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `connectivity`: Connectivity matrices between neural populations.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the BVC activity and rates at the beginning of each simulation step, based on the current dynamics and mode of operation.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.k_ratio.bvc = (\n",
    "            - self.activity.bvc\n",
    "            + self.connectivity.bvc.to.bvc['phi'] + self.weights.bvc.to.bvc @ self.rates.bvc\n",
    "            + self.connectivity.ovc.to.bvc['phi']*self.weights.ovc.to.bvc @ self.rates.ovc\n",
    "            + self.connectivity.pr.to.bvc['phi']*self.weights.pr.to.bvc @ self.rates.pr\n",
    "        )\n",
    "\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                self.k_ratio.bvc += self.connectivity.tr.to.bvc['phi']*self.weights.tr.to.bvc @ np.sum(self.rates.tr, axis=0)\n",
    "            case 'recall' | 'top-down':\n",
    "                self.k_ratio.bvc += self.connectivity.h.to.bvc['phi']*self.weights.h.to.bvc @ self.rates.h\n",
    "\n",
    "        self.activity.bvc += self.dt/self.connectivity.bvc.to.bvc['tau']*self.k_ratio.bvc\n",
    "\n",
    "        self.rates.bvc = 1/(1 + np.exp(-2*self.connectivity.bvc.to.bvc['beta']*(self.activity.bvc - self.connectivity.bvc.to.bvc['alpha'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVCCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update the Object Vector Cells (OVC) based on various inputs and connectivity in an agent-based learning simulation.\n",
    "\n",
    "    This callback computes the activity and rates of OVCs by integrating inputs from different sources according to the current mode of the simulation.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to compute the updates for OVCs, including dynamics parameters, connectivity, weights, and rates from other cell types.\n",
    "    \"\"\"\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating OVCs, and prepares for computation.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the activity and rates of OVCs at the beginning of each simulation step, based on the current dynamics parameters and inputs from various sources.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.k_ratio.ovc = (\n",
    "            - self.activity.ovc\n",
    "            + self.connectivity.ovc.to.ovc['phi']*self.weights.ovc.to.ovc @ self.rates.ovc\n",
    "            + self.connectivity.bvc.to.ovc['phi']*self.weights.bvc.to.ovc @ self.rates.bvc\n",
    "        )\n",
    "\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                self.k_ratio.ovc += self.connectivity.tr.to.ovc['phi']*self.weights.tr.to.bvc @ np.sum(self.rates.otr, axis=0)\n",
    "            case 'recall' | 'top-down':\n",
    "                self.k_ratio.ovc += (\n",
    "                    self.connectivity.opr.to.ovc['phi']*self.weights.opr.to.ovc @ self.rates.opr\n",
    "                    + self.connectivity.h.to.ovc['phi']*self.weights.h.to.ovc @ self.rates.h\n",
    "                )\n",
    "\n",
    "        self.activity.ovc += self.dt/self.connectivity.ovc.to.ovc['tau']*self.k_ratio.ovc\n",
    "\n",
    "        self.rates.ovc = 1/(1 + np.exp(-2*self.connectivity.ovc.to.ovc['beta']*(self.activity.ovc - self.connectivity.ovc.to.ovc['alpha'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_cue(env: Environment, walls_fov: list[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a perirhinal (PR) cue vector based on the visibility and distance of walls within the field of view.\n",
    "\n",
    "    Args:\n",
    "        env (Environment): The simulation environment containing walls and their textures.\n",
    "        walls_fov (list[np.ndarray]): A list of numpy arrays, each representing the points of a wall that are within the field of view.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The PR cue vector, where each element represents a normalized count of points for each texture, adjusted by the minimum distance to the observer.\n",
    "    \"\"\"\n",
    "    # 1. get amount of points for each texture id\n",
    "    counts = dict()\n",
    "    distances = dict()\n",
    "    for wall, wall_fov in zip(env.walls, walls_fov):\n",
    "        if wall.polygon.texture.id_ not in counts:\n",
    "            counts[wall.polygon.texture.id_] = len(wall_fov)\n",
    "            d = np.sqrt(np.sum(wall_fov**2, axis=1))\n",
    "            if d.size:\n",
    "                distances[wall.polygon.texture.id_] = np.min(d)\n",
    "            else:\n",
    "                distances[wall.polygon.texture.id_] = np.inf\n",
    "        else:\n",
    "            counts[wall.polygon.texture.id_] += len(wall_fov)\n",
    "            distances[wall.polygon.texture.id_] = min(distances[wall.polygon.texture.id_], np.min(np.sqrt(np.sum(wall_fov**2, axis=1))))\n",
    "\n",
    "    distances = dict(sorted(distances.items(), key=lambda x: x[0])) # walls with smallest id first\n",
    "    counts = dict(sorted(counts.items(), key=lambda x: x[0]))\n",
    "    # 2. get the pr cue vector which is counts normalized by the distance\n",
    "    pr_cue = np.array([count/dist for count, dist in zip(counts.values(), distances.values())])\n",
    "\n",
    "    return pr_cue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update perirhinal cortex (PR) related parameters and activities in an agent-based learning simulation.\n",
    "\n",
    "    This callback computes the PR cue from environmental inputs, updates the k-ratio for PR neurons based on various inputs and connectivity, and finally updates the PR neuron rates.\n",
    "\n",
    "    Attributes:\n",
    "        pr_cue_scale (float): A scaling factor for the PR cue to adjust its influence on the PR neuron activities.\n",
    "    \"\"\"\n",
    "    def __init__(self, pr_cue_scale: float = 50):\n",
    "        \"\"\"\n",
    "        Initializes the PRCallback instance with a specified PR cue scale.\n",
    "\n",
    "        Args:\n",
    "            pr_cue_scale (float): The scaling factor for the PR cue.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pr_cue_scale = pr_cue_scale\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating PR parameters, and prepares for PR cue computation.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `env`: The simulation environment containing walls and their textures.\n",
    "            - `walls_fov`: A list of numpy arrays, each representing the points of a wall that are within the field of view.\n",
    "            - `k_ratio`: Ratio of excitation/inhibition for PR cells.\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `connectivity`: Connectivity matrices between neural populations.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'env',\n",
    "            'walls_fov',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the PR cue, k-ratio, activity, and rates at the beginning of each simulation step, based on the current environmental inputs and connectivity.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        pr_cue = get_pr_cue(self.env, self.walls_fov)[:, np.newaxis]\n",
    "\n",
    "        self.pr_cue = self.pr_cue_scale*pr_cue/np.max(pr_cue)\n",
    "\n",
    "        self.k_ratio.pr = -self.activity.pr +\\\n",
    "            self.connectivity.pr.to.pr['phi']*self.weights.pr.to.pr @ self.rates.pr +\\\n",
    "            self.connectivity.bvc.to.pr['phi']*self.weights.bvc.to.pr @ self.rates.bvc +\\\n",
    "            self.pr_cue\n",
    "\n",
    "        if self.dynamics_params['mode'] == 'recall' or self.dynamics_params['mode'] == 'top-down':\n",
    "            self.k_ratio.pr += self.connectivity.h.to.pr['phi']*self.weights.h.to.pr @ self.rates.h\n",
    "\n",
    "        self.activity.pr += self.dt/self.connectivity.pr.to.pr['tau']*self.k_ratio.pr\n",
    "\n",
    "        self.rates.pr = 1/(1 + np.exp(-2*self.connectivity.pr.to.pr['beta']*(self.activity.pr - self.connectivity.pr.to.pr['alpha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oPRCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update the oPR (object perirhinal) neuron activations based on environmental cues and internal states in an agent-based learning simulation.\n",
    "\n",
    "    This callback integrates various inputs, including bottom-up sensory information, recall cues, and attentional focus, to update the oPR neuron activations, which are crucial for object recognition and memory recall processes.\n",
    "\n",
    "    Attributes:\n",
    "        opr_cue_scale (float): The scaling factor for the cue signal to the oPR neurons, defaulting to 200.\n",
    "    \"\"\"\n",
    "    def __init__(self, opr_cue_scale: float = 200):\n",
    "        \"\"\"\n",
    "        Initializes the oPRCallback instance with a specified cue scale.\n",
    "\n",
    "        Args:\n",
    "            opr_cue_scale (float, optional): The scaling factor for the cue signal to the oPR neurons. Defaults to 200.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.opr_cue = None\n",
    "        self.opr_cue_scale = opr_cue_scale\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating oPR neuron activations, and initializes the oPR cue array.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'env',\n",
    "            'dynamics_params',\n",
    "            'encoding_params',\n",
    "            'attention_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.opr_cue = np.zeros((len(self.env.objects), 1))\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the oPR neuron activations at the beginning of each simulation step, based on the current dynamics mode, attention parameters, and encoding parameters.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.opr_cue *= 0\n",
    "\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                if self.attention_params['attend_to'] is not None:\n",
    "                    self.opr_cue[self.attention_params['attend_to']] = 1\n",
    "            case 'recall':\n",
    "                self.opr_cue[self.encoding_params['object_to_recall']] = 1\n",
    "\n",
    "        self.k_ratio.opr = (\n",
    "                -self.activity.opr +\n",
    "                self.connectivity.opr.to.opr['phi']*self.weights.opr.to.opr @ self.rates.opr +\n",
    "                self.connectivity.h.to.opr['phi']*self.weights.h.to.opr @ self.rates.h +\n",
    "                self.connectivity.ovc.to.opr['phi']*self.weights.ovc.to.opr @ self.rates.ovc +\n",
    "                self.opr_cue_scale*self.opr_cue\n",
    "        )\n",
    "\n",
    "        self.activity.opr  = self.dt/self.connectivity.pr.to.pr['tau']*self.k_ratio.opr\n",
    "\n",
    "        self.rates.opr = 1/(1 + np.exp(-2*self.connectivity.pr.to.pr['beta']*(self.activity.opr - self.connectivity.pr.to.pr['alpha'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed for updating the Parietal Window (PW) related parameters in an agent-based learning simulation.\n",
    "\n",
    "    This callback computes the activity and rates of the PW based on the current dynamics of the simulation, including interactions with environmental cues and internal cognitive processes.\n",
    "\n",
    "    Attributes:\n",
    "        b_cue_scale (float): The scaling factor for boundary cues in the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, b_cue_scale: float = 48):\n",
    "        \"\"\"\n",
    "        Initializes the PWCallback instance with a specified scaling factor for boundary cues.\n",
    "\n",
    "        Args:\n",
    "            b_cue_scale (float): The scaling factor for boundary cues in the environment.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.b_cue_scale = b_cue_scale\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating PW parameters, and prepares for computation.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `k_ratio`: Ratio of excitation/inhibition for PW cells.\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `connectivity`: Connectivity matrices between neural populations.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "            - `walls_pw`: The boundary cues in the environment.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates',\n",
    "            'walls_pw'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the PW activity and rates at the beginning of each simulation step, based on the current mode of operation and environmental interactions.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.k_ratio.pw = (\n",
    "            -self.activity.pw\n",
    "            - self.connectivity.pw.to.pw['inhibitory_phi']\n",
    "        )\n",
    "\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                self.k_ratio.pw += self.b_cue_scale*np.sum(self.walls_pw, axis=0)[:, np.newaxis]\n",
    "            case 'recall' | 'top-down':\n",
    "                self.k_ratio.pw += self.connectivity.tr.to.pw['phi']*np.sum(np.transpose(self.weights.tr.to.pw, (2, 0, 1)) @ self.rates.tr, axis=0)\n",
    "\n",
    "        self.activity.pw += self.dt/self.connectivity.pw.to.pw['tau']*self.k_ratio.pw\n",
    "\n",
    "        self.rates.pw = 1/(1 + np.exp(-2*self.connectivity.pw.to.pw['beta']*(self.activity.pw - self.connectivity.pw.to.pw['alpha'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oPWCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update the oPW (Object Parietal Window) related parameters and activities in an agent-based learning simulation.\n",
    "\n",
    "    This callback handles the computation of oPW activity based on the current dynamics mode (bottom-up, recall, top-down), attention parameters, and object cues.\n",
    "\n",
    "    Attributes:\n",
    "        o_cue_scale (float): The scaling factor for object cues in the oPW computation.\n",
    "        attn_prev (Any): The previous attention target, used to log changes in attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, o_cue_scale: float = 40):\n",
    "        \"\"\"\n",
    "        Initializes the oPWCallback instance with a specified object cue scale.\n",
    "\n",
    "        Args:\n",
    "            o_cue_scale (float, optional): The scaling factor for object cues. Defaults to 40.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn_prev = None\n",
    "        self.o_cue_scale = o_cue_scale\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for oPW computation, and prepares for the simulation step.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `k_ratio`: Ratio of excitation/inhibition for oPW cells.\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `connectivity`: Connectivity matrices between neural populations.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "            - `attention_params`: The attentional focus parameters.\n",
    "            - `objects_pw`: The object cues in the oPW computation.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates',\n",
    "            'attention_params',\n",
    "            'objects_pw'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the oPW activity and rates at the beginning of each simulation step, based on the current dynamics mode and attention parameters.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.k_ratio.opw = (\n",
    "            -self.activity.opw\n",
    "            - np.sum(self.rates.opw) * self.connectivity.opw.to.opw['inhibitory_phi']\n",
    "        )\n",
    "\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                objects_pw_cue = self.o_cue_scale*self.objects_pw[self.attention_params['attend_to']][:, np.newaxis] if self.attention_params['attend_to'] is not None else 0\n",
    "                self.k_ratio.opw += objects_pw_cue\n",
    "            case 'recall' | 'top-down':\n",
    "                self.k_ratio.opw += self.connectivity.tr.to.opw['phi']*np.sum(np.transpose(self.weights.tr.to.pw, (2, 0, 1)) @ self.rates.otr, axis=0)\n",
    "\n",
    "        self.activity.opw += self.dt/self.connectivity.opw.to.opw['tau']*self.k_ratio.opw\n",
    "\n",
    "        self.rates.opw = 1/(1 + np.exp(-2*self.connectivity.opw.to.opw['beta']*(self.activity.opw - self.connectivity.opw.to.opw['alpha'])))\n",
    "\n",
    "        if self.attn_prev != self.attention_params['attend_to']:\n",
    "            self.attn_prev = self.attention_params['attend_to']\n",
    "            logging.debug(f'Switch attention to {self.attention_params[\"attend_to\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "\n",
    "class IPRateCallback(BaseCallback):\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        self.requires = [\n",
    "            'connectivity',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        self.rates.ip = np.array([1/(1 + np.exp(-2*self.connectivity.ip.to.ip['beta']*(self.connectivity.hd.to.ip['phi']*np.sum(self.rates.hd) - self.connectivity.ip.to.ip['alpha'])))])\n",
    "\n",
    "\n",
    "class TCCallback(BaseCallback):\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "\n",
    "        self.k_ratio.tr = (\n",
    "            -self.activity.tr\n",
    "            - np.sum(self.rates.tr, axis=0) * self.connectivity.tr.to.tr['inhibitory_phi']\n",
    "            + self.connectivity.hd.to.tr['phi']*np.transpose(self.weights.hd.to.tr, (2, 0, 1)) @ self.rates.hd\n",
    "            - self.connectivity.ip.to.tr['phi']*self.rates.ip\n",
    "        )\n",
    "\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                self.k_ratio.tr += self.connectivity.pw.to.tr['phi']*np.transpose(self.weights.pw.to.tr, (2, 0, 1)) @ self.rates.pw\n",
    "            case 'recall' | 'top-down':\n",
    "                self.k_ratio.tr += self.connectivity.bvc.to.tr['phi']*self.weights.bvc.to.tr @ self.rates.bvc\n",
    "\n",
    "        self.activity.tr += self.dt/self.connectivity.tr.to.tr['tau']*self.k_ratio.tr\n",
    "\n",
    "        self.rates.tr = 1/(1 + np.exp(-2*self.connectivity.tr.to.tr['beta']*(self.activity.tr - self.connectivity.tr.to.tr['alpha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oTCCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed for updating the transformation circuit (oTC) for objects in an agent-based learning simulation.\n",
    "\n",
    "    This callback computes the activity and rates of the oTC neurons based on various inputs and connectivity parameters, adjusting for the current dynamics of the simulation.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to compute the updates for the oTC neurons.\n",
    "    \"\"\"\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating oTC neurons, and prepares for computation.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `dynamics_params`: Contains dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "            - `k_ratio`: Ratio of excitation/inhibition for oTC cells.\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `connectivity`: Connectivity matrices between neural populations.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'dynamics_params',\n",
    "            'k_ratio',\n",
    "            'activity',\n",
    "            'connectivity',\n",
    "            'weights',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.dt = self.dynamics_params['dt']\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the oTC neurons' activity and rates at the beginning of each simulation step, based on the current mode of operation and connectivity parameters.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.k_ratio.otr = (\n",
    "            - self.activity.otr\n",
    "            - np.sum(self.rates.otr, axis=0) * self.connectivity.otr.to.otr['inhibitory_phi']\n",
    "            + self.connectivity.hd.to.tr['phi']*np.transpose(self.cache['weights'].hd.to.tr, (2, 0, 1)) @ self.rates.hd\n",
    "            - self.connectivity.ip.to.otr['phi']*self.rates.ip\n",
    "        )\n",
    "\n",
    "        match self.dynamics_params['mode']:\n",
    "            case 'bottom-up':\n",
    "                self.k_ratio.otr += self.connectivity.opw.to.tr['phi']*np.transpose(self.cache['weights'].pw.to.tr, (2, 0, 1)) @ self.rates.opw\n",
    "            case 'recall' | 'top-down':\n",
    "                self.k_ratio.otr += self.connectivity.bvc.to.tr['phi']*self.weights.bvc.to.tr @ self.rates.ovc\n",
    "\n",
    "\n",
    "        self.activity.otr += self.dt/self.connectivity.otr.to.otr['tau']*self.k_ratio.otr\n",
    "\n",
    "        self.rates.otr = 1/(1 + np.exp(-2*self.connectivity.tr.to.tr['beta']*(self.activity.otr - self.connectivity.tr.to.tr['alpha'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectWeightsUpdatingCallback(BaseCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_steps: int = 10,\n",
    "        rate_threshold: float = .99\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_steps = init_steps\n",
    "        self.rate_threshold = rate_threshold\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        if 'encoded_objects' not in cache['encoding_params'] or\\\n",
    "            cache['encoding_params']['encoded_objects'] is None:\n",
    "            cache['encoding_params'].update(dict(\n",
    "                encoded_objects=DirectedTensorGroup(\n",
    "                    DirectedTensor(\n",
    "                        from_='opr',\n",
    "                        to='ovc',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='opr',\n",
    "                        to='h',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='opr',\n",
    "                        to='hd',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='bvc',\n",
    "                        to='ovc',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='ovc',\n",
    "                        to='h',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    )\n",
    "                )\n",
    "            ))\n",
    "        self.requires = [\n",
    "            'activity',\n",
    "            'weights',\n",
    "            'rates',\n",
    "            'encoding_params',\n",
    "            'attention_params',\n",
    "            'dynamics_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_step_end(self, step: int):\n",
    "        if self.dynamics_params.mode == 'bottom-up' and self.dynamics_params['step'] > self.init_steps:\n",
    "            ovc_rate_max = self.rates.ovc.max()\n",
    "            ovc_act = np.maximum(self.activity.ovc, 0)\n",
    "            # not np.all(ovc_act < self.ovc_act_threshold) and\\\n",
    "            if self.attention_params['attend_to'] is not None and\\\n",
    "                ovc_rate_max > self.rate_threshold and\\\n",
    "                self.cache.attention_params.attention_step > self.cache.attention_params.attention_cycle//3:\n",
    "\n",
    "                if not self.encoding_params['encoded_objects'].ovc.to.h[self.attention_params['attend_to']]:\n",
    "                    h_act = np.maximum(self.activity.h, 0)\n",
    "                    ovc_act[ovc_act < 0.05] = 0\n",
    "                    ovc_x_h_act = h_act @ ovc_act.T\n",
    "                    max_act = np.max(ovc_x_h_act)\n",
    "                    if np.isclose(max_act, 0):\n",
    "                        max_act = 1\n",
    "                    ovc_x_h_act /= max_act\n",
    "                    tmp1 = ovc_x_h_act > 0.05\n",
    "                    self.weights.ovc.to.h[tmp1] += ovc_x_h_act[tmp1]\n",
    "                    self.weights.h.to.ovc = self.weights.ovc.to.h.T\n",
    "                    self.encoding_params['encoded_objects'].ovc.to.h[self.attention_params['attend_to']] = True\n",
    "                    logging.debug(f'OVC2H FOR OBJECT {self.attention_params[\"attend_to\"]} UPDATED')\n",
    "\n",
    "                # FIXME: Wow this structure seems a bit messy\n",
    "                if not self.encoding_params['encoded_objects'].opr.to.ovc[self.attention_params['attend_to']]:\n",
    "                    ovc_act = np.maximum(self.activity.ovc, 0)\n",
    "                    opr_act = np.maximum(self.activity.opr, 0)\n",
    "                    ovc_act[ovc_act < 0.05] = 0\n",
    "                    ovc_x_opr_act = opr_act @ ovc_act.T\n",
    "                    max_act = np.max(ovc_x_opr_act)\n",
    "                    if np.isclose(max_act, 0):\n",
    "                        max_act = 1\n",
    "                    ovc_x_opr_act = ovc_x_opr_act / max_act\n",
    "                    tmp1 = ovc_x_opr_act > 0.05\n",
    "                    self.weights.ovc.to.opr[tmp1] += ovc_x_opr_act[tmp1]\n",
    "                    self.weights.opr.to.ovc = self.weights.ovc.to.opr.T\n",
    "                    self.encoding_params['encoded_objects'].opr.to.ovc[self.attention_params['attend_to']] = True\n",
    "                    logging.debug(f'OPR2OVC FOR OBJECT {self.attention_params[\"attend_to\"]} UPDATED')\n",
    "\n",
    "                if not self.encoding_params['encoded_objects'].opr.to.h[self.attention_params['attend_to']]:\n",
    "                    h_act = np.maximum(self.activity.h, 0)\n",
    "                    opr_act = np.maximum(self.activity.opr, 0)\n",
    "                    h_x_opr_act = opr_act @ h_act.T\n",
    "                    max_act = np.max(h_x_opr_act)\n",
    "                    if np.isclose(max_act, 0):\n",
    "                        max_act = 1\n",
    "                    h_x_opr_act = h_x_opr_act / max_act\n",
    "                    opr_x_h_act = h_x_opr_act.T\n",
    "                    tmp1 = opr_x_h_act > 0.2\n",
    "                    self.weights.opr.to.h[tmp1] += opr_x_h_act[tmp1]\n",
    "                    self.weights.h.to.opr = self.weights.opr.to.h.T\n",
    "                    self.encoding_params['encoded_objects'].opr.to.h[self.attention_params['attend_to']] = True\n",
    "                    logging.debug(f'OPR2H FOR OBJECT {self.attention_params[\"attend_to\"]} UPDATED')\n",
    "\n",
    "                if not self.encoding_params['encoded_objects'].opr.to.hd[self.attention_params['attend_to']]:\n",
    "                    hd_act = np.maximum(self.activity.hd, 0)\n",
    "                    hd_x_opr_act = opr_act @ hd_act.T\n",
    "                    opr_x_hd_act = hd_x_opr_act.T\n",
    "                    max_act = np.max(opr_x_hd_act)\n",
    "                    if np.isclose(max_act, 0):\n",
    "                        max_act = 1\n",
    "                    opr_x_hd_act = opr_x_hd_act / max_act\n",
    "                    tmp1 = opr_x_hd_act > 0.2\n",
    "                    self.weights.opr.to.hd[tmp1] += opr_x_hd_act[tmp1]\n",
    "                    self.encoding_params['encoded_objects'].opr.to.hd[self.attention_params['attend_to']] = True\n",
    "                    logging.debug(f'OPR2HD FOR OBJECT {self.attention_params[\"attend_to\"]} UPDATED')\n",
    "\n",
    "                if not self.encoding_params['encoded_objects'].bvc.to.ovc[self.attention_params['attend_to']]:\n",
    "                    bvc_act = np.maximum(self.activity.bvc, 0)\n",
    "                    bvc_act[bvc_act < 0.05] = 0\n",
    "                    ovc_act = np.maximum(self.activity.ovc, 0)\n",
    "                    bvc_x_ovc_act = ovc_act @ bvc_act.T\n",
    "                    max_act = np.max(bvc_x_ovc_act)\n",
    "                    if np.isclose(max_act, 0):\n",
    "                        max_act = 1\n",
    "                    bvc_x_ovc_act = bvc_x_ovc_act / max_act\n",
    "                    tmp1 = bvc_x_ovc_act > 0.07\n",
    "                    self.weights.bvc.to.ovc[tmp1] += bvc_x_ovc_act[tmp1]\n",
    "                    self.encoding_params['encoded_objects'].bvc.to.ovc[self.attention_params['attend_to']] = True\n",
    "                    logging.debug(f'BVC2OVC FOR OBJECT {self.attention_params[\"attend_to\"]} UPDATED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class ObjectWeightsUpdatingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update the synaptic weights between different objects-related populations of neurons based on their activity levels during the bottom-up processing mode of an agent-based learning simulation.\n",
    "\n",
    "    This callback dynamically adjusts the connections between neurons to reflect the learning process as the agent interacts with its environment.\n",
    "\n",
    "    Attributes:\n",
    "        init_steps (int): The number of initial steps to ignore before starting the weight updating process.\n",
    "        rate_threshold (float): The threshold for the maximum rate of a neuron population to consider an object as attended.\n",
    "        population_thresholds (DirectedTensorGroup): Custom thresholds for updating weights between different neuron populations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_steps: int = 10,\n",
    "        rate_threshold: float = .99,\n",
    "        population_thresholds: DirectedTensorGroup = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ObjectWeightsUpdatingCallback with specified initial steps, rate threshold, and population thresholds.\n",
    "\n",
    "        Args:\n",
    "            init_steps (int, optional): The number of initial steps to ignore before starting the weight updating process. Defaults to 10.\n",
    "            rate_threshold (float, optional): The threshold for the maximum rate of a neuron population to consider an object as attended. Defaults to .99.\n",
    "            population_thresholds (DirectedTensorGroup, optional): Custom thresholds for updating weights between different neuron populations. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.init_steps = init_steps\n",
    "        self.rate_threshold = rate_threshold\n",
    "        if population_thresholds is None:\n",
    "            self.population_thresholds = DirectedTensorGroup(\n",
    "                DirectedTensor(\n",
    "                    from_='ovc',\n",
    "                    to='h',\n",
    "                    weights=dict(threshold=0.05, update_threshold=0.05)\n",
    "                ),\n",
    "                DirectedTensor(\n",
    "                    from_='ovc',\n",
    "                    to='opr',\n",
    "                    weights=dict(threshold=0.05, update_threshold=0.05)\n",
    "                ),\n",
    "                DirectedTensor(\n",
    "                    from_='h',\n",
    "                    to='opr',\n",
    "                    weights=dict(threshold=None, update_threshold=0.2)\n",
    "                ),\n",
    "                DirectedTensor(\n",
    "                    from_='hd',\n",
    "                    to='opr',\n",
    "                    weights=dict(threshold=None, update_threshold=0.2)\n",
    "                ),\n",
    "                DirectedTensor(\n",
    "                    from_='bvc',\n",
    "                    to='ovc',\n",
    "                    weights=dict(threshold=0.05, update_threshold=0.07)\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes encoded objects if not present, and specifies the required cache keys for updating weights.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "\n",
    "        Notes:\n",
    "            The `self.requires` attribute specifies the following required parameters:\n",
    "            - `activity`: Neural activity levels.\n",
    "            - `weights`: Synaptic weights between neurons.\n",
    "            - `rates`: Firing rates of neurons.\n",
    "            - `encoding_params`: Parameters related to encoding objects in the simulation.\n",
    "            - `attention_params`: Parameters related to the attentional focus of the agent.\n",
    "            - `dynamics_params`: Dynamic parameters of the simulation, including the current mode (e.g., 'bottom-up', 'top-down', 'recall').\n",
    "        \"\"\"\n",
    "        if 'encoded_objects' not in cache['encoding_params'] or\\\n",
    "            cache['encoding_params']['encoded_objects'] is None:\n",
    "            cache['encoding_params'].update(dict(\n",
    "                encoded_objects=DirectedTensorGroup(\n",
    "                    DirectedTensor(\n",
    "                        from_='ovc',\n",
    "                        to='h',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='ovc',\n",
    "                        to='opr',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='h',\n",
    "                        to='opr',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='hd',\n",
    "                        to='opr',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    ),\n",
    "                    DirectedTensor(\n",
    "                        from_='bvc',\n",
    "                        to='ovc',\n",
    "                        weights=np.zeros(len(cache['env'].objects)).astype(bool)\n",
    "                    )\n",
    "                )\n",
    "            ))\n",
    "        self.requires = [\n",
    "            'activity',\n",
    "            'weights',\n",
    "            'rates',\n",
    "            'encoding_params',\n",
    "            'attention_params',\n",
    "            'dynamics_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def update_weights_and_encoding(\n",
    "        self,\n",
    "        from_act: TensorGroup,\n",
    "        to_act: TensorGroup,\n",
    "        from_key: str, to_key: str,\n",
    "        threshold: float = 0.05,\n",
    "        update_threshold: float = 0.2,\n",
    "        update_rule: Literal['forward', 'backward', 'bidirectional'] = 'bidirectional'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the synaptic weights and encoding status between specified neuron populations based on their activity levels and thresholds.\n",
    "\n",
    "        Args:\n",
    "            from_act (TensorGroup): The activity levels of the source neuron population.\n",
    "            to_act (TensorGroup): The activity levels of the target neuron population.\n",
    "            from_key (str): The key identifying the source neuron population.\n",
    "            to_key (str): The key identifying the target neuron population.\n",
    "            threshold (float, optional): The activity threshold for considering a neuron as active. Defaults to 0.05.\n",
    "            update_threshold (float, optional): The threshold for updating synaptic weights. Defaults to 0.2.\n",
    "            update_rule (Literal['forward', 'backward', 'bidirectional'], optional): The direction of the connection between populations which weights are going to be updated. Defaults to 'bidirectional'.\n",
    "        \"\"\"\n",
    "        if not self.encoding_params['encoded_objects'][from_key].to[to_key][self.attention_params['attend_to']]:\n",
    "            from_act = np.maximum(from_act, 0)\n",
    "            to_act = np.maximum(to_act, 0)\n",
    "\n",
    "            if threshold is not None:\n",
    "                from_act[from_act < threshold] = 0\n",
    "\n",
    "            act_product = to_act @ from_act.T\n",
    "            max_act = np.max(act_product)\n",
    "            if np.isclose(max_act, 0):\n",
    "                max_act = 1\n",
    "            act_product /= max_act\n",
    "\n",
    "            match update_rule:\n",
    "                case 'bidirectional':\n",
    "                    significant_act = act_product > update_threshold\n",
    "                    update = act_product[significant_act]\n",
    "                    self.weights[from_key].to[to_key][significant_act] += update\n",
    "                    self.weights[to_key].to[from_key] = self.weights[from_key].to[to_key].T\n",
    "                    print(from_key, to_key, self.weights[to_key].to[from_key].max(), self.weights[from_key].to[to_key].max())\n",
    "                case 'forward':\n",
    "                    significant_act = act_product > update_threshold\n",
    "                    update = act_product[significant_act]\n",
    "                    self.weights[from_key].to[to_key][significant_act] += update\n",
    "                case 'backward':\n",
    "                    act_product = act_product.T\n",
    "                    significant_act = act_product > update_threshold\n",
    "                    update = act_product[significant_act]\n",
    "                    self.weights[to_key].to[from_key][significant_act] += update\n",
    "\n",
    "            self.encoding_params['encoded_objects'][from_key].to[to_key][self.attention_params['attend_to']] = True\n",
    "            logging.debug(f'{from_key.upper()}2{to_key.upper()} FOR OBJECT {self.attention_params[\"attend_to\"]} UPDATED')\n",
    "\n",
    "    def on_step_end(self, step: int):\n",
    "        \"\"\"\n",
    "        Executes the weight updating logic at the end of each simulation step, based on the current mode of operation and activity levels.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if self.dynamics_params.mode == 'bottom-up' and self.dynamics_params['step'] > self.init_steps:\n",
    "            ovc_rate_max = self.rates.ovc.max()\n",
    "            if self.attention_params['attend_to'] is not None and\\\n",
    "                ovc_rate_max > self.rate_threshold and\\\n",
    "                self.cache.attention_params.attention_step > self.cache.attention_params.attention_cycle//3:\n",
    "\n",
    "                self.update_weights_and_encoding(\n",
    "                    self.activity.ovc, self.activity.h,\n",
    "                    'ovc', 'h',\n",
    "                    self.population_thresholds.ovc.to.h['threshold'],\n",
    "                    self.population_thresholds.ovc.to.h['update_threshold']\n",
    "                )\n",
    "                self.update_weights_and_encoding(\n",
    "                    self.activity.ovc, self.activity.opr,\n",
    "                    'ovc', 'opr',\n",
    "                    self.population_thresholds.ovc.to.opr['threshold'],\n",
    "                    self.population_thresholds.ovc.to.opr['update_threshold']\n",
    "                )\n",
    "                self.update_weights_and_encoding(\n",
    "                    self.activity.h, self.activity.opr,\n",
    "                    'h', 'opr',\n",
    "                    self.population_thresholds.h.to.opr['threshold'],\n",
    "                    self.population_thresholds.h.to.opr['update_threshold']\n",
    "                )\n",
    "                self.update_weights_and_encoding(\n",
    "                    self.activity.hd, self.activity.opr,\n",
    "                    'hd', 'opr',\n",
    "                    self.population_thresholds.hd.to.opr['threshold'],\n",
    "                    self.population_thresholds.hd.to.opr['update_threshold'],\n",
    "                    update_rule='backward'\n",
    "                )\n",
    "                self.update_weights_and_encoding(\n",
    "                    self.activity.bvc, self.activity.ovc,\n",
    "                    'bvc', 'ovc',\n",
    "                    self.population_thresholds.bvc.to.ovc['threshold'],\n",
    "                    self.population_thresholds.bvc.to.ovc['update_threshold']\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "from bbtoolkit.structures import BaseCallbacksManager, BaseCallback as _BaseCallback, CallbacksCollection\n",
    "from bbtoolkit.utils.viz import show_figure\n",
    "\n",
    "\n",
    "class ArtistCallback(_BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback class designed for handling plotting-related tasks within a simulation or iterative process.\n",
    "\n",
    "    This class extends the _BaseCallback class, providing specific methods for plotting and cleaning up plots.\n",
    "    \"\"\"\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Called to execute the plotting logic.\n",
    "        \"\"\"\n",
    "        ...\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Called to clean up or clear figures before plotting anew.\n",
    "        \"\"\"\n",
    "        ...\n",
    "    def on_copy(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when the callback is copied.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "        \"\"\"\n",
    "        ...\n",
    "    def on_load(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when the callback is loaded from a serialized state.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "class PlottingCallback(BaseCallbacksManager, BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback class that manages plotting callbacks and integrates with the BaseCallbacksManager.\n",
    "\n",
    "    This class is responsible for managing artist callbacks, updating plots at a specified rate, and handling figure and grid specifications for plotting.\n",
    "\n",
    "    This class acts as both a BaseCallback and a BaseCallbacksManager, allowing it to manage a collection of ArtistCallback and execute them in a specific order, while being a callback of a parent BaseCallbacksManager.\n",
    "\n",
    "    Attributes:\n",
    "        update_rate (int): The rate at which the plot should be updated.\n",
    "        fig_kwargs (dict): Keyword arguments for figure creation.\n",
    "        gc_kwargs (dict): Keyword arguments for grid specification.\n",
    "\n",
    "    Inherits:\n",
    "        BaseCallbacksManager: For managing a collection of callbacks.\n",
    "        BaseCallback: For callback functionality.\n",
    "    \"\"\"\n",
    "    def __init__(self, callbacks: list[ArtistCallback] = None, update_rate: int = 10, fig_kwargs: dict = None, gc_kwargs: dict = None):\n",
    "        \"\"\"\n",
    "        Initializes the PlottingCallback instance.\n",
    "\n",
    "        Args:\n",
    "            callbacks (list[ArtistCallback], optional): A list of ArtistCallback instances.\n",
    "            update_rate (int, optional): The rate at which the plot should be updated. Defaults to 10.\n",
    "            fig_kwargs (dict, optional): Keyword arguments for figure creation. Defaults to None.\n",
    "            gc_kwargs (dict, optional): Keyword arguments for grid specification. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.update_rate = update_rate\n",
    "        fig_kwargs = fig_kwargs or dict()\n",
    "        gc_kwargs = gc_kwargs or dict()\n",
    "        self.fig_kwargs = fig_kwargs\n",
    "        self.gc_kwargs = gc_kwargs\n",
    "        self.callbacks = CallbacksCollection() if callbacks is None else CallbacksCollection(callbacks)\n",
    "        BaseCallback.__init__(self)\n",
    "\n",
    "    @property\n",
    "    def cache(self):\n",
    "        \"\"\"\n",
    "        Returns the current cache.\n",
    "\n",
    "        Returns:\n",
    "            Mapping: The current cache.\n",
    "        \"\"\"\n",
    "        return self._cache\n",
    "\n",
    "    @cache.setter\n",
    "    def cache(self, cache: Mapping):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "        \"\"\"\n",
    "        self._cache = cache\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and initializes figure and grid specifications.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = ['fig', 'gc']\n",
    "        if 'fig' not in cache or not isinstance(cache['fig'], plt.Figure):\n",
    "            cache['fig'] = plt.figure(**self.fig_kwargs)\n",
    "            cache['gc'] = GridSpec(**self.gc_kwargs, figure=cache['fig'])\n",
    "        elif 'fig' in cache and isinstance(cache['fig'], plt.Figure):\n",
    "            cache['fig'].clf()\n",
    "        else:\n",
    "            raise ValueError(f'Invalid cache key for fig: {type(cache[\"fig\"])}')\n",
    "\n",
    "        super().set_cache(cache, on_repeat=on_repeat)\n",
    "        self.callbacks.execute('set_cache', cache, on_repeat=on_repeat)\n",
    "        try:\n",
    "            self.callbacks.validate()\n",
    "        except TypeError as e:\n",
    "            raise TypeError(\n",
    "                f'Error in {self.__class__.__name__}: Failed to validate callbacks due to: {e}\\n'\n",
    "                f'Note: {self.__class__.__name__} acts as both a BaseCallback and a BaseCallbacksManager.\\n'\n",
    "                f'This means that callbacks within {self.__class__.__name__} are nested within the scope of any external callbacks manager utilizing {self.__class__.__name__}.\\n'\n",
    "                'As a result, these nested callbacks have their own separate visibility scope.\\n'\n",
    "                f'If these nested callbacks depend on cache keys available in the external callbacks manager’s cache, they must be positioned before {self.__class__.__name__} in the execution order.'\n",
    "            )\n",
    "\n",
    "        cache['fig'].tight_layout()\n",
    "\n",
    "    def validate_fig(self):\n",
    "        \"\"\"\n",
    "        Validates the existence of the figure and displays it if not already visible.\n",
    "        \"\"\"\n",
    "        if not plt.fignum_exists(self.fig.number):\n",
    "            show_figure(self.fig)\n",
    "\n",
    "    def on_step_end(self, step: int):\n",
    "        \"\"\"\n",
    "        Called at the end of a step. Updates the plot based on the update rate.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if not step % self.update_rate:\n",
    "            self.plot()\n",
    "\n",
    "    def on_simulation_end(self):\n",
    "        \"\"\"\n",
    "        Called at the end of the simulation. Closes the plot.\n",
    "        \"\"\"\n",
    "        plt.close(self.fig)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Validates the figure, cleans up previous plots, executes plotting callbacks, and refreshes the plot.\n",
    "        \"\"\"\n",
    "        self.validate_fig()\n",
    "        self.callbacks.execute('on_clean')\n",
    "        self.callbacks.execute('on_plot')\n",
    "        self.cache['fig'].canvas.draw()\n",
    "        plt.pause(.00001)\n",
    "\n",
    "    def on_copy(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when the callback is copied.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "        \"\"\"\n",
    "        self.callbacks.validate()\n",
    "        self.callbacks.execute('on_copy', **kwargs)\n",
    "\n",
    "    def on_load(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Called when the callback is loaded from a serialized state.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "        \"\"\"\n",
    "        self.callbacks.validate()\n",
    "        self.callbacks.execute('on_load', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "import matplotlib.colors as mcolors\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "from bbtoolkit.utils.viz.colors import adjust_color_brightness, get_most_visible_color\n",
    "\n",
    "\n",
    "class AloEnvPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the environment in an agent-based learning simulation.\n",
    "\n",
    "    This callback handles the visualization of the environment, including walls, objects, and the agent's field of view (FOV).\n",
    "\n",
    "    Attributes:\n",
    "        attn_color (str): The color for the agent's attention.\n",
    "        min_xy (tuple): Minimum x and y coordinates for the plot boundaries.\n",
    "        max_xy (tuple): Maximum x and y coordinates for the plot boundaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_color: str = 'tab:red'):\n",
    "        \"\"\"\n",
    "        Initializes the AloEnvPlotter instance.\n",
    "\n",
    "        Args:\n",
    "            attn_color (str): The color for the agent's attention. Defaults to 'tab:red'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn_color = attn_color\n",
    "        self.min_xy = None\n",
    "        self.max_xy = None\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and initializes the plot axis for the environment.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['alo_ax'] = cache['fig'].add_subplot(cache['gc'][:4, :4])\n",
    "        self.requires = [\n",
    "            'env',\n",
    "            'walls_fov',\n",
    "            'objects_fov',\n",
    "            'alo_ax',\n",
    "            'attention_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        coords_x, coords_y = self.env.visible_area.boundary.coords.xy\n",
    "        min_train_x, max_train_x, min_train_y, max_train_y = min(coords_x), max(coords_x), min(coords_y), max(coords_y)\n",
    "        self.min_xy = (min_train_x, min_train_y)\n",
    "        self.max_xy = (max_train_x, max_train_y)\n",
    "\n",
    "    def plot_environment(self):\n",
    "        \"\"\"\n",
    "        Plots the environment, including walls and objects.\n",
    "        \"\"\"\n",
    "        for obj in self.env.objects + self.env.walls:\n",
    "            plot_polygon(obj.polygon, ax=self.alo_ax, alpha=0.5, linewidth=1)\n",
    "\n",
    "    def plot_fov(self):\n",
    "        \"\"\"\n",
    "        Plots the agent's field of view, showing visible walls and objects.\n",
    "        \"\"\"\n",
    "        if self.walls_fov:\n",
    "            for wall, poly in zip(self.walls_fov, self.env.walls):\n",
    "                self.alo_ax.plot(\n",
    "                    wall[:, 0],\n",
    "                    wall[:, 1],\n",
    "                    'o',\n",
    "                    color=poly.polygon.texture.color,\n",
    "                    markersize=2,\n",
    "                    markeredgecolor=adjust_color_brightness(poly.polygon.texture.color)\n",
    "                )\n",
    "        if self.objects_fov:\n",
    "            for i, (obj, poly) in enumerate(zip(self.objects_fov, self.env.objects)):\n",
    "                if self.attention_params['attend_to'] is not None and i == self.attention_params['attend_to']:\n",
    "                    self.alo_ax.plot(\n",
    "                        obj[:, 0],\n",
    "                        obj[:, 1],\n",
    "                        'o',\n",
    "                        color=self.attn_color,\n",
    "                        markersize=3,\n",
    "                        markeredgecolor=adjust_color_brightness(self.attn_color)\n",
    "                    )\n",
    "                else:\n",
    "                    self.alo_ax.plot(\n",
    "                        obj[:, 0],\n",
    "                        obj[:, 1],\n",
    "                        'o',\n",
    "                        color=poly.polygon.texture.color,\n",
    "                        markersize=2,\n",
    "                        markeredgecolor=adjust_color_brightness(poly.polygon.texture.color)\n",
    "                    )\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic, including the environment and the agent's field of view.\n",
    "        \"\"\"\n",
    "        self.plot_environment()\n",
    "        self.plot_fov()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the plot in preparation for the next update, and sets the axis limits based on the environment boundaries.\n",
    "        \"\"\"\n",
    "        self.alo_ax.clear()\n",
    "        self.alo_ax.set_axis_off()\n",
    "        self.alo_ax.set_xlim(self.min_xy[0], self.max_xy[0])\n",
    "        self.alo_ax.set_ylim(self.min_xy[1], self.max_xy[1])\n",
    "\n",
    "\n",
    "class TargetPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting movement and rotation targets in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes targets for movement and rotation, aiding in understanding the agent's intended actions.\n",
    "\n",
    "    Attributes:\n",
    "        move_target_color (str): The color used to plot the movement target. Defaults to 'tab:red'.\n",
    "        rotate_target_color (str): The color used to plot the rotation target. Defaults to 'tab:green'.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        move_target_color: str = 'tab:red',\n",
    "        rotate_target_color: str = 'tab:green'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the TargetPlotter instance with specified colors for movement and rotation targets.\n",
    "\n",
    "        Args:\n",
    "            move_target_color (str, optional): The color for movement targets. Defaults to 'tab:red'.\n",
    "            rotate_target_color (str, optional): The color for rotation targets. Defaults to 'tab:green'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.move_target_color = move_target_color\n",
    "        self.rotate_target_color = rotate_target_color\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and specifies the required cache keys for plotting targets.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'alo_ax',\n",
    "            'movement_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for movement and rotation targets, using specified colors for each.\n",
    "        \"\"\"\n",
    "        if self.movement_params.move_target is not None:\n",
    "            self.alo_ax.plot(*self.movement_params.move_target, 'x', color=self.move_target_color, zorder=3.5)\n",
    "        if self.movement_params.rotate_target is not None:\n",
    "            self.alo_ax.plot(*self.movement_params.rotate_target, 'x', color=self.rotate_target_color, zorder=3.5)\n",
    "\n",
    "\n",
    "class AgentPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the agent's position and direction in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's current position and the direction it is facing, aiding in understanding the agent's state and intended actions.\n",
    "\n",
    "    Attributes:\n",
    "        agent_color (str): The color used to plot the agent. Defaults to 'tab:blue'.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent_color: str = 'tab:blue'):\n",
    "        \"\"\"\n",
    "        Initializes the AgentPlotter instance with a specified color for the agent.\n",
    "\n",
    "        Args:\n",
    "            agent_color (str, optional): The color for the agent. Defaults to 'tab:blue'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.agent_color = agent_color\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and specifies the required cache keys for plotting the agent.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'alo_ax',\n",
    "            'movement_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the agent's position and direction, using the specified color for the agent.\n",
    "        \"\"\"\n",
    "        if self.movement_params.position is not None and self.movement_params.direction is not None:\n",
    "            self.alo_ax.plot(*self.movement_params.position, 'o', color=self.agent_color, zorder=4.5)\n",
    "            self.alo_ax.arrow(\n",
    "                *self.movement_params.position,\n",
    "                0.5 * math.cos(self.movement_params.direction),\n",
    "                0.5 * math.sin(self.movement_params.direction),\n",
    "                zorder=4.5\n",
    "            )\n",
    "\n",
    "\n",
    "class TrajectoryPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the agent's trajectory and movement targets in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the path that the agent has taken or will take, along with the final target position, aiding in understanding the agent's movement strategy.\n",
    "\n",
    "    Attributes:\n",
    "        traj_color (str): The color used to plot the trajectory. Defaults to 'tab:green'.\n",
    "        target_color (str): The color used to plot the final target position. Defaults to 'tab:red'.\n",
    "    \"\"\"\n",
    "    def __init__(self, traj_color: str = 'tab:green', target_color: str = 'tab:red'):\n",
    "        \"\"\"\n",
    "        Initializes the TrajectoryPlotter instance with specified colors for the trajectory and target.\n",
    "\n",
    "        Args:\n",
    "            traj_color (str, optional): The color for the trajectory. Defaults to 'tab:green'.\n",
    "            target_color (str, optional): The color for the final target position. Defaults to 'tab:red'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.traj_color = traj_color\n",
    "        self.target_color = target_color\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and specifies the required cache keys for plotting the trajectory and target.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'alo_ax',\n",
    "            'movement_params',\n",
    "            'movement_schedule',\n",
    "            'trajectory'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the agent's trajectory and movement target, using specified colors for each.\n",
    "        \"\"\"\n",
    "        if self.movement_params.position is not None and \\\n",
    "            (not len(self.trajectory) or\n",
    "            not (\n",
    "                self.movement_params.move_target is not None\n",
    "                and self.movement_params.move_target not in self.trajectory\n",
    "            )):\n",
    "            first_points = [self.movement_params.position, self.movement_params.move_target]\\\n",
    "                if self.movement_params.move_target not in self.movement_schedule\\\n",
    "                and self.movement_params.move_target is not None\\\n",
    "                else [self.movement_params.position]\n",
    "            all_points = first_points + self.movement_schedule\n",
    "            if len(self.movement_schedule):\n",
    "                self.alo_ax.plot(\n",
    "                    self.movement_schedule[-1][0],\n",
    "                    self.movement_schedule[-1][1],\n",
    "                    'X', color=self.target_color,\n",
    "                    zorder=3.5\n",
    "                )\n",
    "            for from_, to in zip(all_points[:-1], all_points[1:]):\n",
    "                self.alo_ax.plot(*zip(from_, to), '-', color=self.traj_color, alpha=.5, zorder=2.5)\n",
    "\n",
    "\n",
    "class EgoEnvPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting an ego-centric view of the environment in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the environment from the agent's perspective, including walls and objects, with special attention to objects of interest.\n",
    "\n",
    "    Attributes:\n",
    "        attn_color (str): The color used to highlight objects of interest. Defaults to 'tab:red'.\n",
    "        min_xy (tuple): Minimum x and y coordinates for the plot boundaries.\n",
    "        max_xy (tuple): Maximum x and y coordinates for the plot boundaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_color: str = 'tab:red'):\n",
    "        \"\"\"\n",
    "        Initializes the EgoEnvPlotter instance with a specified color for highlighting objects of interest.\n",
    "\n",
    "        Args:\n",
    "            attn_color (str, optional): The color for highlighting objects of interest. Defaults to 'tab:red'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attn_color = attn_color\n",
    "        self.min_xy = None\n",
    "        self.max_xy = None\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and initializes the plot axis for the ego-centric view.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['ego_ax'] = cache['fig'].add_subplot(cache['gc'][4:8, :4])\n",
    "        self.requires = [\n",
    "            'env',\n",
    "            'walls_ego_segments',\n",
    "            'objects_ego_segments',\n",
    "            'ego_ax',\n",
    "            'attention_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        coords_x, coords_y = self.env.visible_area.boundary.coords.xy\n",
    "        min_train_x, max_train_x, min_train_y, max_train_y = min(coords_x), max(coords_x), min(coords_y), max(coords_y)\n",
    "        d = np.sqrt((max_train_x - min_train_x)**2 + (max_train_y - min_train_y)**2)\n",
    "        middle = (max_train_x + min_train_x)/2, (max_train_y + min_train_y)/2\n",
    "        self.min_xy = (middle[0] - d, -1)\n",
    "        self.max_xy = (middle[0] + d, d)\n",
    "\n",
    "    def plot_ego(self):\n",
    "        \"\"\"\n",
    "        Plots the ego-centric representation of walls and objects.\n",
    "        \"\"\"\n",
    "        _ = plot_arrow(np.pi/2, 0, -.75, ax=self.ego_ax)\n",
    "\n",
    "        if self.walls_ego_segments:\n",
    "            for segments, poly in zip(self.walls_ego_segments, self.env.walls):\n",
    "                for seg in segments:\n",
    "                    x_start, y_start, x_end, y_end = seg\n",
    "                    self.ego_ax.plot([x_start, x_end], [y_start, y_end], color=poly.polygon.texture.color, linewidth=1)\n",
    "\n",
    "        if self.objects_ego_segments:\n",
    "            for i, (segments, poly) in enumerate(zip(self.objects_ego_segments, self.env.objects)):\n",
    "                color = self.attn_color if self.attention_params['attend_to'] is not None and i == self.attention_params['attend_to'] else poly.polygon.texture.color\n",
    "                for seg in segments:\n",
    "                    x_start, y_start, x_end, y_end = seg\n",
    "                    self.ego_ax.plot([x_start, x_end], [y_start, y_end], color=color, linewidth=1)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the ego-centric view of the environment.\n",
    "        \"\"\"\n",
    "        self.plot_ego()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the plot in preparation for the next update, sets the axis ticks and limits based on the ego-centric view boundaries.\n",
    "        \"\"\"\n",
    "        self.ego_ax.clear()\n",
    "        self.ego_ax.set_xticks([])\n",
    "        self.ego_ax.set_yticks([])\n",
    "\n",
    "        for spine in self.ego_ax.spines.values():\n",
    "            spine.set_edgecolor('tab:grey')\n",
    "\n",
    "        self.ego_ax.set_xlim(self.min_xy[0], self.max_xy[0])\n",
    "        self.ego_ax.set_ylim(self.min_xy[1], self.max_xy[1])\n",
    "\n",
    "\n",
    "class MouseEventCallback(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for handling mouse events within the plotting area of an agent-based learning simulation.\n",
    "\n",
    "    This callback enables interactive setting of movement and rotation targets through mouse clicks on the plot.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to handle mouse events effectively, including the figure object for connecting events and environmental data for determining click positions relative to objects and walls.\n",
    "    \"\"\"\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and connects the mouse click event to the on_click method.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'fig',\n",
    "            'env',\n",
    "            'dynamics_params',\n",
    "            'movement_params',\n",
    "            'mental_movement_params',\n",
    "            'click_params',\n",
    "            'alo_ax'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.fig.canvas.mpl_connect('button_press_event', self.on_click)\n",
    "\n",
    "    @staticmethod\n",
    "    def point_outside_bounds(x, y, objects):\n",
    "        \"\"\"\n",
    "        Determines if a clicked point is outside the bounds of specified objects.\n",
    "\n",
    "        Args:\n",
    "            x (float): The x-coordinate of the clicked point.\n",
    "            y (float): The y-coordinate of the clicked point.\n",
    "            objects (list): A list of objects to check against.\n",
    "\n",
    "        Returns:\n",
    "            bool or np.array: False if the point is outside all objects, otherwise the indices of objects containing the point.\n",
    "        \"\"\"\n",
    "        point = Point(x, y)\n",
    "        is_contained = np.array([\n",
    "            obj.polygon.contains(point)\n",
    "            for obj in objects\n",
    "        ])\n",
    "        if not np.any(is_contained):\n",
    "            return False\n",
    "        else:\n",
    "            return np.where(is_contained)[0]\n",
    "\n",
    "    def on_click(self, event: MouseEvent):\n",
    "        \"\"\"\n",
    "        Handles mouse click events on the plot for setting movement and rotation targets.\n",
    "\n",
    "        Args:\n",
    "            event (MouseEvent): The mouse click event on the plot.\n",
    "        \"\"\"\n",
    "\n",
    "        if event.inaxes is self.alo_ax:\n",
    "\n",
    "            self.click_params['xy_data'] = (event.xdata, event.ydata)\n",
    "            self.click_params['inside_object'] = self.point_outside_bounds(event.xdata, event.ydata, self.env.objects)\n",
    "            self.click_params['inside_wall'] = self.point_outside_bounds(event.xdata, event.ydata, self.env.walls)\n",
    "\n",
    "            match self.dynamics_params['mode']:\n",
    "                case 'bottom-up':\n",
    "                    movement_params = self.movement_params\n",
    "                case 'recall' | 'top-down':\n",
    "                    movement_params = self.mental_movement_params\n",
    "\n",
    "            if self.dynamics_params['mode'] == 'recall' and\\\n",
    "                (\n",
    "                    (\n",
    "                        self.click_params['inside_object'] is False\n",
    "                        and self.click_params['inside_wall'] is False\n",
    "                    ) or event.button is MouseButton.RIGHT\n",
    "                ):\n",
    "                    logging.debug('Switching to top-down mode')\n",
    "                    self.dynamics_params['mode'] = 'top-down'\n",
    "\n",
    "            # Be aware of checking self.click_params['inside_object'], since it can be either false of np.array which may not survive if-else (in the case of np.array([0]))\n",
    "            if event.button is MouseButton.LEFT and self.click_params['inside_object'] is False and self.click_params['inside_wall'] is False:\n",
    "                movement_params.move_target = event.xdata, event.ydata\n",
    "                movement_params.rotate_target = None\n",
    "            elif event.button is MouseButton.RIGHT:\n",
    "                movement_params.rotate_target = event.xdata, event.ydata\n",
    "                movement_params.move_target = None\n",
    "\n",
    "    def on_copy(self):\n",
    "        \"\"\"\n",
    "        Reconnects the mouse click event handler when the callback is copied.\n",
    "        \"\"\"\n",
    "        self.fig.canvas.mpl_connect('button_press_event', self.on_click)\n",
    "\n",
    "    def on_load(self):\n",
    "        \"\"\"\n",
    "        Reconnects the mouse click event handler when the callback is loaded from a serialized state.\n",
    "        \"\"\"\n",
    "        self.on_copy()\n",
    "\n",
    "\n",
    "class TimerCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to keep track of simulation steps within an agent-based learning simulation.\n",
    "\n",
    "    This callback updates a step counter in the dynamics parameters each time a simulation step ends, providing a mechanism to monitor the progress of the simulation.\n",
    "\n",
    "    Inherits:\n",
    "        BaseCallback: For basic callback functionality.\n",
    "    \"\"\"\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and initializes the step counter in the dynamics parameters.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = ['dynamics_params']\n",
    "        cache['dynamics_params']['step'] = 0\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_step_end(self, step: int):\n",
    "        \"\"\"\n",
    "        Increments the step counter in the dynamics parameters at the end of each simulation step.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        self.dynamics_params['step'] += 1\n",
    "\n",
    "\n",
    "class TimerPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the current simulation time on the plot in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the simulation time, providing a real-time update on the plot as the simulation progresses.\n",
    "\n",
    "    Attributes:\n",
    "        coords (tuple): Coordinates on the plot where the simulation time text will be displayed.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the TimerPlotter instance.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.coords = None\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for plotting the time, and calculates the coordinates for displaying the time text.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = ['alo_ax', 'dynamics_params', 'env']\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        x, y = cache.env.visible_area.boundary.xy\n",
    "        self.coords = min(x), max(y) + 1\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the current simulation time on the plot.\n",
    "        \"\"\"\n",
    "        self.alo_ax.text(*self.coords, f'Time: {(self.dynamics_params.step)*self.dynamics_params.dt : .2f} s')\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for displaying the current simulation time.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "\n",
    "class PWPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting Parietal Window (PW) representations in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's perception of walls in a polar coordinate system, providing insights into the agent's navigational state and environmental understanding.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to plot the PW representation, including a dedicated axis for polar plotting and data related to the agent's perception and the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap: str | mcolors.Colormap = 'coolwarm'):\n",
    "        \"\"\"\n",
    "        Initializes the PWPlotter instance with a specified colormap for the polar plot.\n",
    "\n",
    "        Args:\n",
    "            cmap (str | mcolors.Colormap, optional): The colormap for the polar plot. Defaults to 'coolwarm'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cmap = cmap\n",
    "        self.grid_color = get_most_visible_color(self.cmap)\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the polar plot axis, and calculates necessary parameters for plotting the PW representation.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['pw_ax'] = cache.fig.add_subplot(cache.gc[4:8, 4:8], projection='polar')\n",
    "        self.requires = [\n",
    "            'pw_ax',\n",
    "            'rates',\n",
    "            'env',\n",
    "            'walls_pw',\n",
    "            'tc_gen',\n",
    "            'theta_bvc',\n",
    "            'r_bvc'\n",
    "        ]\n",
    "        polar_distance = calculate_polar_distance(cache.tc_gen.r_max)\n",
    "        polar_angle = np.linspace(\n",
    "            0,\n",
    "            (cache.tc_gen.n_bvc_theta + 1) * cache.tc_gen.polar_ang_res,\n",
    "            cache.tc_gen.n_bvc_theta\n",
    "        )\n",
    "        polar_distance, polar_angle = np.meshgrid(polar_distance, polar_angle)\n",
    "        cache['theta_bvc'], cache['r_bvc'] = np.meshgrid(\n",
    "            np.linspace(0, 2 * np.pi, cache.tc_gen.n_bvc_theta),  # Angular dimension\n",
    "            np.linspace(0, 1, cache.tc_gen.n_bvc_r)  # Radial dimension, adjust as necessary\n",
    "        )\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the ego-centric representation of walls and objects.\n",
    "        \"\"\"\n",
    "        self.pw_ax.contourf(\n",
    "            self.theta_bvc.T,\n",
    "            self.r_bvc.T,\n",
    "            np.reshape(np.maximum(self.rates.pw, 1e-7), (self.tc_gen.n_bvc_theta, self.tc_gen.n_bvc_r)),\n",
    "            cmap=self.cmap,\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the polar wall representation.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the polar plot in preparation for the next update and sets up the axis labels and ticks for better readability.\n",
    "        \"\"\"\n",
    "        self.pw_ax.clear()\n",
    "        self.pw_ax.set_yticklabels([])\n",
    "        self.pw_ax.set_xticklabels([])\n",
    "        self.pw_ax.grid(color=self.grid_color)\n",
    "        self.pw_ax.set_theta_zero_location('E')\n",
    "        self.pw_ax.set_xticks(np.linspace(0, 2*np.pi, 4, endpoint=False))\n",
    "        self.pw_ax.set_xticklabels(['Right', 'Straight', 'Left', 'Back'])\n",
    "\n",
    "\n",
    "class BVCPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting Boundary Vector Cell (BVC) representations in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's BVC activations in a polar coordinate system, providing insights into the agent's spatial cognition and environmental boundaries perception.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to plot the BVC representation, including a dedicated axis for polar plotting and data related to the agent's spatial cognition and the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap: str | mcolors.Colormap = 'coolwarm'):\n",
    "        \"\"\"\n",
    "        Initializes the BVCPlotter instance with a specified colormap for the polar plot.\n",
    "\n",
    "        Args:\n",
    "            cmap (str | mcolors.Colormap, optional): The colormap for the polar plot. Defaults to 'coolwarm'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cmap = cmap\n",
    "        self.grid_color = get_most_visible_color(self.cmap)\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the polar plot axis for BVC representation, and specifies the required cache keys for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['bvc_ax'] = cache.fig.add_subplot(cache.gc[:4, 4:8], projection='polar')\n",
    "        self.requires = [\n",
    "            'bvc_ax',\n",
    "            'rates',\n",
    "            'env',\n",
    "            'walls_pw',\n",
    "            'tc_gen',\n",
    "            'theta_bvc',\n",
    "            'r_bvc'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the ego-centric representation of walls and objects.\n",
    "        \"\"\"\n",
    "        self.bvc_ax.contourf(\n",
    "            self.theta_bvc.T,\n",
    "            self.r_bvc.T,\n",
    "            np.reshape(np.maximum(self.rates.bvc, 1e-7), (self.tc_gen.n_bvc_theta, self.tc_gen.n_bvc_r)),\n",
    "            cmap=self.cmap,\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the BVC representation.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the polar plot in preparation for the next update and sets up the axis labels and ticks for navigational directions.\n",
    "        \"\"\"\n",
    "        self.bvc_ax.clear()\n",
    "        self.bvc_ax.set_yticklabels([])\n",
    "        self.bvc_ax.set_xticklabels([])\n",
    "        self.bvc_ax.grid(color=self.grid_color)\n",
    "        self.bvc_ax.set_theta_zero_location('S')\n",
    "        self.bvc_ax.set_xticks(np.linspace(0, 2*np.pi, 4, endpoint=False))\n",
    "        self.bvc_ax.set_xticklabels(['N', 'E', 'S', 'W'])\n",
    "\n",
    "\n",
    "class oPWPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the Parietal Window (oPW) for objects in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's perception of objects within its environment in a polar coordinate system, specifically through the lens of the oPW, providing insights into the agent's object-related spatial cognition.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to plot the oPW representation, including a dedicated axis for polar plotting and data related to the agent's perception of objects and the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap: str | mcolors.Colormap = 'coolwarm'):\n",
    "        \"\"\"\n",
    "        Initializes the oPWPlotter instance with a specified colormap for the polar plot.\n",
    "\n",
    "        Args:\n",
    "            cmap (str | mcolors.Colormap, optional): The colormap for the polar plot. Defaults to 'coolwarm'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cmap = cmap\n",
    "        self.grid_color = get_most_visible_color(self.cmap)\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the polar plot axis for oPW representation, and specifies the required cache keys for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['opw_ax'] = cache.fig.add_subplot(cache.gc[4:8, 8:], projection='polar')\n",
    "        self.requires = [\n",
    "            'opw_ax',\n",
    "            'rates',\n",
    "            'env',\n",
    "            'walls_pw',\n",
    "            'tc_gen',\n",
    "            'theta_bvc',\n",
    "            'r_bvc'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the ego-centric representation of walls and objects.\n",
    "        \"\"\"\n",
    "        self.opw_ax.contourf(\n",
    "            self.theta_bvc.T,\n",
    "            self.r_bvc.T,\n",
    "            np.reshape(np.maximum(self.rates.opw, 1e-7), (self.tc_gen.n_bvc_theta, self.tc_gen.n_bvc_r)),\n",
    "            cmap=self.cmap,\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the oPW representation.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the polar plot in preparation for the next update and sets up the axis labels and ticks for navigational directions related to object perception.\n",
    "        \"\"\"\n",
    "        self.opw_ax.clear()\n",
    "        self.opw_ax.set_yticklabels([])\n",
    "        self.opw_ax.set_xticklabels([])\n",
    "        self.opw_ax.grid(color=self.grid_color)\n",
    "        self.opw_ax.set_theta_zero_location('E')\n",
    "        self.opw_ax.set_xticks(np.linspace(0, 2*np.pi, 4, endpoint=False))\n",
    "        self.opw_ax.set_xticklabels(['Right', 'Straight', 'Left', 'Back'])\n",
    "\n",
    "\n",
    "class OVCPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the Object Vector Cells (OVC) representation in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's OVC activations in a polar coordinate system, providing insights into the agent's object-related spatial cognition and how it perceives objects within the environment.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to plot the OVC representation, including a dedicated axis for polar plotting and data related to the agent's perception of objects and the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap: str | mcolors.Colormap = 'coolwarm'):\n",
    "        \"\"\"\n",
    "        Initializes the OVCPlotter instance with a specified colormap for the polar plot.\n",
    "\n",
    "        Args:\n",
    "            cmap (str | mcolors.Colormap, optional): The colormap for the polar plot. Defaults to 'coolwarm'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cmap = cmap\n",
    "        self.grid_color = get_most_visible_color(self.cmap)\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the polar plot axis for OVC representation, and specifies the required cache keys for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['ovc_ax'] = cache['fig'].add_subplot(cache['gc'][:4, 8:], projection='polar')\n",
    "        self.requires = [\n",
    "            'ovc_ax',\n",
    "            'rates',\n",
    "            'env',\n",
    "            'walls_pw',\n",
    "            'tc_gen',\n",
    "            'theta_bvc',\n",
    "            'r_bvc'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the ego-centric representation of walls and objects.\n",
    "        \"\"\"\n",
    "        self.ovc_ax.contourf(\n",
    "            self.theta_bvc.T,\n",
    "            self.r_bvc.T,\n",
    "            np.reshape(np.maximum(self.rates.ovc, 1e-7), (self.tc_gen.n_bvc_theta, self.tc_gen.n_bvc_r)),\n",
    "            cmap=self.cmap,\n",
    "            vmin=0, vmax=1\n",
    "        )\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the OVC representation.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the polar plot in preparation for the next update and sets up the axis labels and ticks for navigational directions related to object perception.\n",
    "        \"\"\"\n",
    "        self.ovc_ax.clear()\n",
    "        self.ovc_ax.set_yticklabels([])\n",
    "        self.ovc_ax.set_xticklabels([])\n",
    "        self.ovc_ax.grid(color=self.grid_color)\n",
    "        self.ovc_ax.set_theta_zero_location('S')\n",
    "        self.ovc_ax.set_xticks(np.linspace(0, 2*np.pi, 4, endpoint=False))\n",
    "        self.ovc_ax.set_xticklabels(['N', 'E', 'S', 'W'])\n",
    "\n",
    "\n",
    "class HDPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting Head Direction (HD) cell activations in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's HD cell activations, providing insights into the agent's directional orientation and spatial cognition.\n",
    "\n",
    "    Attributes:\n",
    "        cmap (str): The colormap used for plotting HD cell activations. Defaults to 'coolwarm'.\n",
    "        theta (np.ndarray): The angular positions for each HD cell activation.\n",
    "        kwargs (dict): Additional keyword arguments for plotting.\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap: str | mcolors.Colormap = 'coolwarm', **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the HDPlotter instance with a specified colormap and additional plotting arguments.\n",
    "\n",
    "        Args:\n",
    "            cmap (str | mcolors.Colormap, optional): The colormap for HD cell activations. Defaults to 'coolwarm'.\n",
    "            **kwargs: Arbitrary keyword arguments for plotting.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.theta = None\n",
    "        self.cmap = cmap\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the plotting axis for HD representation, and specifies the required cache keys for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['hd_ax'] = cache['fig'].add_subplot(cache['gc'][8:, 8:])\n",
    "        self.requires = [\n",
    "            'hd_ax',\n",
    "            'rates'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.theta = np.linspace(0, 2*np.pi, len(self.rates.hd), endpoint=False)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the HD cell activations using a scatter plot to visualize the agent's directional orientation.\n",
    "        \"\"\"\n",
    "        for i, (shift, r) in enumerate(zip((1, 8, 4), (1, .95, .9))):\n",
    "\n",
    "            vis = self.rates.hd.copy()\n",
    "            if i == 1:\n",
    "                th_indices = np.where(vis > .1)[0]\n",
    "\n",
    "                breakdown = np.where(np.diff(th_indices) != 1)[0]\n",
    "                if breakdown.size:\n",
    "                    th_indices = np.concatenate([th_indices[breakdown[0]+1:], th_indices[:breakdown[0]+1], ])\n",
    "\n",
    "                prev = vis[((i)+th_indices[len(th_indices)//2])%len(vis)]\n",
    "                for i in range(len(th_indices)//2):\n",
    "                    this_index = ((i)+th_indices[len(th_indices)//2])%len(vis)\n",
    "                    next_index = (this_index + 1)%len(vis)\n",
    "                    temp = vis[next_index].copy()\n",
    "                    vis[next_index] = prev\n",
    "                    prev = temp.copy()\n",
    "\n",
    "            self.hd_ax.scatter(\n",
    "                r*np.cos(self.theta - 2*np.pi/shift),\n",
    "                r*np.sin(self.theta - 2*np.pi/shift),\n",
    "                marker='.',\n",
    "                c=np.roll(vis, int(1/shift*len(self.rates.hd))),\n",
    "                s=1.5*r*(plt.rcParams['lines.markersize']**2),\n",
    "                cmap=self.cmap,\n",
    "                **self.kwargs\n",
    "            )\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the HD cell activations.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the plot in preparation for the next update and sets up the axis for better readability and orientation indication.\n",
    "        \"\"\"\n",
    "        self.hd_ax.clear()\n",
    "        self.hd_ax.set_axis_off()\n",
    "        self.hd_ax.set_aspect('equal')\n",
    "        self.hd_ax.text(1.2, 0, '0°', ha='center', va='center')  # Add a label at the top\n",
    "        self.hd_ax.text(0, 1.2, '90°', ha='center', va='center')  # Add a label at the top\n",
    "        self.hd_ax.text(-1.2, 0, '180°', ha='center', va='center')  # Add a label at the top\n",
    "        self.hd_ax.text(0, -1.2, '270°', ha='center', va='center')  # Add a label at the top\n",
    "        self.hd_ax.vlines(0, -.8, .8, color='tab:grey', linewidth=1, alpha=.8)\n",
    "        self.hd_ax.hlines(0, -.8, .8, color='tab:grey', linewidth=1, alpha=.8)\n",
    "        self.hd_ax.plot(.2*np.cos(self.theta), .2*np.sin(self.theta), color='tab:grey', linewidth=1, alpha=.8)\n",
    "        self.hd_ax.plot(.5*np.cos(self.theta), .5*np.sin(self.theta), color='tab:grey', linewidth=1, alpha=.8)\n",
    "\n",
    "\n",
    "class PCPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting Place Cell (PC) activations in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's PC activations, providing insights into the agent's spatial location and cognitive map of the environment.\n",
    "\n",
    "    Attributes:\n",
    "        shape (tuple): The shape of the grid to which PC activations are mapped.\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap: str | mcolors.Colormap = 'coolwarm'):\n",
    "        \"\"\"\n",
    "        Initializes the PCPlotter instance.\n",
    "\n",
    "        Args:\n",
    "            cmap (str | mcolors.Colormap, optional): The colormap for PC activations. Defaults to 'coolwarm'.\n",
    "        \"\"\"\n",
    "        self.shape = None\n",
    "        self.cmap = cmap\n",
    "        super().__init__()\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the plotting axis for PC representation, and specifies the required cache keys for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        cache['gc_ax'] = cache.fig.add_subplot(cache.gc[8:, :4])\n",
    "        self.requires = [\n",
    "            'gc_ax',\n",
    "            'rates',\n",
    "            'grid2cart'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.shape = self.cache.grid2cart.shape\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the PC activations using an image plot to visualize the agent's spatial location within the environment.\n",
    "        \"\"\"\n",
    "        self.gc_ax.imshow(\n",
    "            np.reshape(self.rates.h, self.shape),\n",
    "            cmap=self.cmap,\n",
    "            vmin=0, vmax=1,\n",
    "        )\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the PC activations.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the plot in preparation for the next update and sets up the axis for better readability.\n",
    "        \"\"\"\n",
    "        self.gc_ax.clear()\n",
    "        self.gc_ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class DistanceAttention(RhythmicAttention):\n",
    "    \"\"\"\n",
    "    A subclass of RhythmicAttention that focuses on objects within a certain distance threshold.\n",
    "\n",
    "    This class implements an attention mechanism based on the distance of objects from a reference position, typically the agent's current position.\n",
    "\n",
    "    Attributes:\n",
    "        freq (float): The frequency of attention oscillation.\n",
    "        dt (float): The time step for updates.\n",
    "        n_objects (int): The number of objects to consider for attention.\n",
    "        dist_threshold (float): The distance threshold for an object to be considered within attention range.\n",
    "    \"\"\"\n",
    "    def __init__(self, freq: float, dt: float, n_objects: int, dist_threshold: float):\n",
    "        \"\"\"\n",
    "        Initializes the DistanceAttention instance with specified parameters for frequency, time step, number of objects, and distance threshold.\n",
    "\n",
    "        Args:\n",
    "            freq (float): The frequency of attention oscillation.\n",
    "            dt (float): The time step for updates.\n",
    "            n_objects (int): The number of objects to consider for attention.\n",
    "            dist_threshold (float): The distance threshold for an object to be considered within attention range.\n",
    "        \"\"\"\n",
    "        super().__init__(freq, dt, n_objects)\n",
    "        self.dist_threshold = dist_threshold\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_dist_to_object(object_: np.ndarray, position: tuple[float, float] = (0, 0)) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the mean distance from a given position to all points of an object.\n",
    "\n",
    "        Args:\n",
    "            object_ (np.ndarray): The object represented as a numpy array of points.\n",
    "            position (tuple[float, float], optional): The reference position. Defaults to (0, 0).\n",
    "\n",
    "        Returns:\n",
    "            float: The mean distance from the position to the object.\n",
    "        \"\"\"\n",
    "        return np.mean(np.linalg.norm(object_ - position, axis=1))\n",
    "\n",
    "    def visible_objects(self, objects: list[np.ndarray]) -> list[int]:\n",
    "        \"\"\"\n",
    "        Determines which objects are visible (i.e., have a non-zero size).\n",
    "\n",
    "        Args:\n",
    "            objects (list[np.ndarray]): A list of objects represented as numpy arrays.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: A list indicating the visibility of each object (True for visible, False for not visible).\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            arr.size > 0\n",
    "            and\n",
    "            self.mean_dist_to_object(arr) < self.dist_threshold\n",
    "            for arr in objects\n",
    "        ])\n",
    "\n",
    "class TimerPrinter(BaseCallback):\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        self.time = time.time()\n",
    "\n",
    "    def on_step_end(self, step):\n",
    "        print(f'Time: {(self.dynamics_params[\"step\"])*self.dynamics_params[\"dt\"] : .2f} s, runtime: {time.time() - self.time} s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backend_bases import KeyEvent\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from bbtoolkit.dynamics.callbacks.movement import MovementParameters\n",
    "\n",
    "\n",
    "class ObjectRecallCallback(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to handle object recall interactions in an agent-based learning simulation.\n",
    "\n",
    "    This callback allows for the initiation and termination of object recall processes through keyboard interactions, enabling the simulation to switch between different modes of operation based on user input.\n",
    "\n",
    "    Attributes:\n",
    "        prev_mode (str): The previous mode of the dynamics parameters before initiating recall.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the ObjectRecallCallback instance.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.prev_mode = None\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for object recall, and connects the key press event to the on_press method.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = ['encoding_params', 'click_params', 'dynamics_params', 'fig']\n",
    "        super().set_cache(cache, on_repeat)\n",
    "        self.fig.canvas.mpl_connect('key_press_event', self.on_press)\n",
    "\n",
    "    def validate_encoding(self):\n",
    "        \"\"\"\n",
    "        Validates that the selected object is encoded in the simulation's memory, raising an error if recall is not possible.\n",
    "        \"\"\"\n",
    "        for from_, data_from in self.encoding_params['encoded_objects'].data.items():\n",
    "                for to, data_to in data_from.items():\n",
    "                    if not data_to[self.click_params[\"inside_object\"][0]]:\n",
    "                        raise ValueError(\n",
    "                            f'Object {self.click_params[\"inside_object\"][0]} is not encoded in {from_}2{to} weights. Recall is not possible.'\n",
    "                        )\n",
    "\n",
    "    def on_press(self, event: KeyEvent):\n",
    "        \"\"\"\n",
    "        Handles key press events for initiating and terminating object recall, as well as switching simulation modes.\n",
    "\n",
    "        Args:\n",
    "            event (KeyEvent): The key press event.\n",
    "        \"\"\"\n",
    "        if event.key == 'r' and self.click_params['inside_object'] is not False:\n",
    "\n",
    "            if len(self.click_params['inside_object']) > 1:\n",
    "                    raise ValueError(\n",
    "                        f'Several objects are selected: {self.click_params[\"inside_object\"]}. '\n",
    "                        'Likely, these objects are overlapped. Recall is not possible for multiple objects at the same time'\n",
    "                    )\n",
    "\n",
    "            if self.dynamics_params['mode'] != 'recall' or\\\n",
    "                (\n",
    "                    self.dynamics_params['mode'] == 'recall' and\n",
    "                    self.click_params['inside_object'][0] != self.encoding_params['object_to_recall']\n",
    "                ):\n",
    "                logging.debug(f'Initiate recall for object {self.click_params[\"inside_object\"][0]}')\n",
    "                self.validate_encoding()\n",
    "                if self.dynamics_params['mode'] != 'recall':\n",
    "                    self.prev_mode = self.dynamics_params['mode']\n",
    "                self.dynamics_params['mode'] = 'recall'\n",
    "                self.encoding_params['object_to_recall'] = self.click_params['inside_object'][0]\n",
    "            else:\n",
    "                logging.debug('Stop recall')\n",
    "                self.dynamics_params['mode'] = self.prev_mode\n",
    "                self.encoding_params['object_to_recall'] = None\n",
    "\n",
    "        if event.key == 'p':\n",
    "            logging.debug('Switch to bottom-up mode')\n",
    "            self.dynamics_params['mode'] = 'bottom-up'\n",
    "\n",
    "            if self.encoding_params['object_to_recall'] is not None:\n",
    "                self.encoding_params['object_to_recall'] = None\n",
    "\n",
    "    def on_copy(self):\n",
    "        \"\"\"\n",
    "        Reconnects the key press event handler when the callback is copied.\n",
    "        \"\"\"\n",
    "        self.fig.canvas.mpl_connect('key_press_event', self.on_press)\n",
    "\n",
    "    def on_load(self):\n",
    "        \"\"\"\n",
    "        Reconnects the key press event handler when the callback is loaded from a serialized state.\n",
    "        \"\"\"\n",
    "        self.on_copy()\n",
    "\n",
    "\n",
    "class PickedObjectPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for highlighting picked (selected) objects in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the selection of objects by the user, providing a visual cue for which objects are currently focused or interacted with.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to identify and highlight selected objects within the simulation environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, color: str = 'b', **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the PickedObjectPlotter instance with a specified color and line width for highlighting selected objects.\n",
    "\n",
    "        Args:\n",
    "            color (str, optional): The color used to highlight selected objects. Defaults to 'b'.\n",
    "            **kwargs: Arbitrary keyword arguments to be passed to plot_polygon function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.color = color\n",
    "        kwargs.setdefault('alpha', 0.5)\n",
    "        kwargs.setdefault('linewidth', 5)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for plotting picked objects, and prepares for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = ['alo_ax', 'click_params', 'env']\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for highlighting picked objects, using visual cues such as border color and thickness.\n",
    "        \"\"\"\n",
    "        if self.click_params['inside_object'] is not False:\n",
    "            for obj_ind in self.click_params['inside_object']:\n",
    "                plot_polygon(self.env.objects[obj_ind].polygon, ax=self.alo_ax, alpha=self.kwargs['alpha'], linewidth=self.kwargs['linewidth'], color=self.color, zorder=-1)\n",
    "\n",
    "\n",
    "class DistanceAttentionPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for visualizing the attention radius of an agent in an agent-based learning simulation.\n",
    "\n",
    "    This callback draws a circle around the agent to represent the distance threshold within which objects are considered for attention, aiding in understanding the spatial scope of the agent's attention mechanism.\n",
    "\n",
    "    Attributes:\n",
    "        dist_threshold (float): The distance threshold for the agent's attention.\n",
    "        resolution (int): The resolution of the circle representing the attention radius.\n",
    "        theta (np.ndarray): The angular coordinates used to calculate the circle's perimeter.\n",
    "        x_r (np.ndarray): The x-coordinates of the circle's perimeter.\n",
    "        y_r (np.ndarray): The y-coordinates of the circle's perimeter.\n",
    "    \"\"\"\n",
    "    def __init__(self, dist_threshold: float, resolution: int = 100, color: str = 'r', **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the DistanceAttentionPlotter instance with a specified distance threshold and resolution for the attention circle.\n",
    "\n",
    "        Args:\n",
    "            dist_threshold (float): The distance threshold for the agent's attention.\n",
    "            resolution (int, optional): The resolution of the circle representing the attention radius. Defaults to 100.\n",
    "            color (str, optional): The color used to plot the attention radius. Defaults to 'r'.\n",
    "            **kwargs: Arbitrary keyword arguments to be passed to plot function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dist_threshold = dist_threshold\n",
    "        self.theta = np.linspace(0, 2*np.pi, resolution)\n",
    "        self.x_r = dist_threshold * np.cos(self.theta)\n",
    "        self.y_r = dist_threshold * np.sin(self.theta)\n",
    "        self.color = color\n",
    "        kwargs.setdefault('linestyle', ':')\n",
    "        kwargs.setdefault('linewidth', 1)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for plotting the attention radius, and prepares for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = ['alo_ax', 'ego_ax', 'movement_params', 'env']\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the attention radius, drawing circles around the agent in both allocentric and egocentric views.\n",
    "        \"\"\"\n",
    "        ego_circle = np.stack([\n",
    "            self.x_r + self.movement_params.position[0],\n",
    "            self.y_r + self.movement_params.position[1]\n",
    "        ], axis=1)\n",
    "\n",
    "        alo_circle = np.stack([\n",
    "            self.x_r,\n",
    "            self.y_r\n",
    "        ], axis=1)\n",
    "\n",
    "        self.alo_ax.plot(ego_circle[:, 0], ego_circle[:, 1], color=self.color, **self.kwargs)\n",
    "        self.ego_ax.plot(alo_circle[:, 0], alo_circle[:, 1], color=self.color, **self.kwargs)\n",
    "\n",
    "\n",
    "class MentalPositionCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to update the mental representation of the agent's position and direction during recall in an agent-based learning simulation.\n",
    "\n",
    "    This callback computes the agent's mental position and direction based on the highest activation rates in place and head direction cells, respectively, during the recall mode. It resets these parameters when not in recall mode.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to compute and update the mental position and direction.\n",
    "    \"\"\"\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating mental position and direction, and initializes the mental movement parameters if not present.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'rates',\n",
    "            'grid2cart',\n",
    "            'weights',\n",
    "            'dynamics_params',\n",
    "            'mental_movement_params'\n",
    "        ]\n",
    "\n",
    "        if 'mental_movement_params' not in cache:\n",
    "            cache['mental_movement_params'] = MovementParameters()\n",
    "\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "        self.hd_vector = np.linspace(0, 2*np.pi, len(self.rates.hd))\n",
    "\n",
    "    def get_coords_from_grid(self, i: int, j: int) -> tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Converts grid indices to Cartesian coordinates.\n",
    "\n",
    "        Args:\n",
    "            i (int): The grid row index.\n",
    "            j (int): The grid column index.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: The Cartesian coordinates corresponding to the grid indices.\n",
    "        \"\"\"\n",
    "        return self.cache.grid2cart[i, j]\n",
    "\n",
    "    def get_hd_from_rate(self, rate: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Determines the direction based on the highest activation rate in head direction cells.\n",
    "\n",
    "        Args:\n",
    "            rate (np.ndarray): The activation rates of head direction cells.\n",
    "\n",
    "        Returns:\n",
    "            float: The angle corresponding to the highest activation rate.\n",
    "        \"\"\"\n",
    "        return self.hd_vector[np.argmax(rate)]\n",
    "\n",
    "    def on_step_end(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the mental position and direction of the agent at the end of each simulation step, based on the current mode of operation.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if self.dynamics_params['mode'] == 'recall':\n",
    "            x_grid, y_grid = np.unravel_index(np.argmax(\n",
    "                self.rates.h\n",
    "            ), self.grid2cart.shape)\n",
    "            self.mental_movement_params['position'] = self.get_coords_from_grid(x_grid, y_grid)\n",
    "            self.mental_movement_params['direction'] = self.get_hd_from_rate(self.rates.hd)\n",
    "        elif self.dynamics_params['mode'] == 'bottom-up':\n",
    "            self.mental_movement_params['position'] = None\n",
    "            self.mental_movement_params['direction'] = None\n",
    "\n",
    "\n",
    "class MentalAgentPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the mental representation of the agent's position and direction during recall or top-down processing in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's mental position and direction, providing insights into the agent's internal state and cognitive processes during different modes of operation.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to plot the mental representation of the agent's position and direction.\n",
    "    \"\"\"\n",
    "    def __init__(self, color: str = '#8fbbd9'):\n",
    "        \"\"\"\n",
    "        Initializes the MentalAgentPlotter instance with a specified color for the mental representation of the agent.\n",
    "\n",
    "        Args:\n",
    "            color (str, optional): The color used to plot the mental representation of the agent. Defaults to '#8fbbd9'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.color = color\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for plotting the mental representation of the agent's position and direction, and prepares for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'rates',\n",
    "            'alo_ax',\n",
    "            'env',\n",
    "            'weights',\n",
    "            'dynamics_params',\n",
    "            'mental_movement_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the mental representation of the agent's position and direction, using visual cues such as color and markers.\n",
    "        \"\"\"\n",
    "        if self.dynamics_params['mode'] in ('recall', 'top-down'):\n",
    "            if self.mental_movement_params['position'] is not None:\n",
    "                self.alo_ax.plot(\n",
    "                    *self.mental_movement_params['position'],\n",
    "                    color=self.color,\n",
    "                    marker='o',\n",
    "                    zorder=4.5\n",
    "                )\n",
    "                if self.mental_movement_params['direction'] is not None:\n",
    "                    self.alo_ax.arrow(\n",
    "                        *self.mental_movement_params['position'],\n",
    "                        0.5 * math.cos(self.mental_movement_params['direction'] ),\n",
    "                        0.5 * math.sin(self.mental_movement_params['direction'] ),\n",
    "                        zorder=4.5\n",
    "                    )\n",
    "\n",
    "\n",
    "class MentalTargetPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the mental representation of movement and rotation targets during recall or top-down processing in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's mental targets for movement and rotation, providing insights into the agent's intended actions based on its cognitive processes.\n",
    "\n",
    "    Attributes:\n",
    "        move_target_color (str): The color used to plot the mental movement target. Defaults to '#eea8a9'.\n",
    "        rotate_target_color (str): The color used to plot the mental rotation target. Defaults to '#95cf95'.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        move_target_color: str = '#eea8a9',\n",
    "        rotate_target_color: str = '#95cf95'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the MentalTargetPlotter instance with specified colors for mental movement and rotation targets.\n",
    "\n",
    "        Args:\n",
    "            move_target_color (str, optional): The color for mental movement targets. Defaults to '#eea8a9'.\n",
    "            rotate_target_color (str, optional): The color for mental rotation targets. Defaults to '#95cf95'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.move_target_color = move_target_color\n",
    "        self.rotate_target_color = rotate_target_color\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for plotting the mental targets, and prepares for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'alo_ax',\n",
    "            'mental_movement_params'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the mental representation of movement and rotation targets, using specified colors for each.\n",
    "        \"\"\"\n",
    "        if self.mental_movement_params.move_target is not None:\n",
    "            self.alo_ax.plot(\n",
    "                *self.mental_movement_params.move_target,\n",
    "                'x', color=self.move_target_color,\n",
    "                zorder=3.5\n",
    "            )\n",
    "        if self.mental_movement_params.rotate_target is not None:\n",
    "            self.alo_ax.plot(\n",
    "                *self.mental_movement_params.rotate_target,\n",
    "                'x', color=self.rotate_target_color,\n",
    "                zorder=3.5\n",
    "            )\n",
    "\n",
    "\n",
    "class MentalTrajectoryPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the mental representation of the agent's trajectory and movement targets during recall or top-down processing in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the agent's mental trajectory and intended movement targets, providing insights into the agent's cognitive planning and navigational strategy.\n",
    "\n",
    "    Attributes:\n",
    "        traj_color (str): The color used to plot the mental trajectory. Defaults to 'tab:green'.\n",
    "        target_color (str): The color used to plot the final mental movement target. Defaults to 'tab:red'.\n",
    "    \"\"\"\n",
    "    def __init__(self, traj_color: str = 'tab:green', target_color: str = 'tab:red'):\n",
    "        \"\"\"\n",
    "        Initializes the MentalTrajectoryPlotter instance with specified colors for the mental trajectory and movement targets.\n",
    "\n",
    "        Args:\n",
    "            traj_color (str, optional): The color for the mental trajectory. Defaults to 'tab:green'.\n",
    "            target_color (str, optional): The color for the final mental movement target. Defaults to 'tab:red'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.traj_color = traj_color\n",
    "        self.target_color = target_color\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for plotting the mental trajectory and targets, and prepares for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'alo_ax',\n",
    "            'mental_movement_params',\n",
    "            'mental_movement_schedule',\n",
    "            'mental_trajectory'\n",
    "        ]\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the mental representation of the agent's trajectory and movement targets, using specified colors for each.\n",
    "        \"\"\"\n",
    "        if self.mental_movement_params.position is not None and \\\n",
    "            (not len(self.mental_trajectory) or\n",
    "            not (\n",
    "                self.mental_movement_params.move_target is not None\n",
    "                and self.mental_movement_params.move_target not in self.mental_trajectory\n",
    "            )):\n",
    "            first_points = [self.mental_movement_params.position, self.mental_movement_params.move_target]\\\n",
    "                if self.mental_movement_params.move_target not in self.mental_movement_schedule\\\n",
    "                and self.mental_movement_params.move_target is not None\\\n",
    "                else [self.mental_movement_params.position]\n",
    "            all_points = first_points + self.mental_movement_schedule\n",
    "\n",
    "            if len(self.mental_movement_schedule):\n",
    "                self.alo_ax.plot(\n",
    "                    self.mental_movement_schedule[-1][0],\n",
    "                    self.mental_movement_schedule[-1][1],\n",
    "                    'X', color=self.target_color,\n",
    "                    zorder=3.5\n",
    "                )\n",
    "\n",
    "            for from_, to in zip(all_points[:-1], all_points[1:]):\n",
    "                self.alo_ax.plot(*zip(from_, to), '-', color=self.traj_color, alpha=.5, zorder=2.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalMovementCallback(MovementCallback):\n",
    "    \"\"\"\n",
    "    A specialized callback for handling mental movement and rotation in an agent-based learning simulation during top-down processing.\n",
    "\n",
    "    This callback updates the mental representation of the agent's position and direction based on specified movement and rotation targets, simulating cognitive planning and navigation.\n",
    "\n",
    "    Inherits:\n",
    "        MovementCallback: For basic movement functionalities.\n",
    "\n",
    "    Attributes:\n",
    "        movement_manager (MovementManager): The manager responsible for handling movement computations.\n",
    "    \"\"\"\n",
    "    def __init__(self, movement_manager: MovementManager):\n",
    "        \"\"\"\n",
    "        Initializes the MentalMovementCallback instance with a specified movement manager.\n",
    "\n",
    "        Args:\n",
    "            movement_manager (MovementManager): The manager responsible for handling movement computations.\n",
    "        \"\"\"\n",
    "        super().__init__(movement_manager)\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, specifies the required cache keys for updating mental movement parameters, and prepares for movement computations.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'mental_movement_params',\n",
    "            'dynamics_params',\n",
    "            'grid2cart'\n",
    "        ]\n",
    "\n",
    "        BaseCallback.set_cache(self, cache, on_repeat)\n",
    "        self.dist = self.movement.distance_per_time(self.dynamics_params.dt)\n",
    "        self.ang = self.movement.angle_per_time(self.dynamics_params.dt)\n",
    "\n",
    "    def on_step_begin(self, step: int): # changes position and angle of an agent\n",
    "        \"\"\"\n",
    "        Updates the mental position and direction of the agent at the beginning of each simulation step, based on the current mode of operation and specified targets.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if self.dynamics_params.mode == 'top-down':\n",
    "            if self.mental_movement_params.position is not None and\\\n",
    "                self.mental_movement_params.move_target is not None:\n",
    "                dist = self.movement.compute_distance(self.mental_movement_params.position, self.mental_movement_params.move_target)\n",
    "                if dist <= self.dist:\n",
    "                    self.mental_movement_params.move_target = None\n",
    "\n",
    "            if self.mental_movement_params.direction is not None and\\\n",
    "                self.mental_movement_params.rotate_target is not None:\n",
    "                    ang = self.movement.smallest_angle_between(\n",
    "                        self.mental_movement_params.direction,\n",
    "                        self.movement.get_angle_with_x_axis(\n",
    "                            [\n",
    "                                self.mental_movement_params.rotate_target[0] - self.mental_movement_params.position[0],\n",
    "                                self.mental_movement_params.rotate_target[1] - self.mental_movement_params.position[1]\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                    if ang <= self.ang:\n",
    "                        self.mental_movement_params.rotate_target = None\n",
    "\n",
    "            if self.mental_movement_params.move_target is not None:\n",
    "                self.mental_movement_params.position = self.move_to_target(\n",
    "                    self.mental_movement_params.position,\n",
    "                    self.mental_movement_params.move_target\n",
    "                )\n",
    "                self.mental_movement_params.direction = self.rotate_to_target(\n",
    "                    self.mental_movement_params.position,\n",
    "                    self.mental_movement_params.direction,\n",
    "                    self.mental_movement_params.move_target\n",
    "                )\n",
    "            elif self.mental_movement_params.rotate_target is not None:\n",
    "                self.mental_movement_params.direction = self.rotate_to_target(\n",
    "                    self.mental_movement_params.position,\n",
    "                    self.mental_movement_params.direction,\n",
    "                    self.mental_movement_params.rotate_target\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class MentalMovementSchedulerCallback(MovementSchedulerCallback):\n",
    "    \"\"\"\n",
    "    A specialized callback for scheduling mental movements in an agent-based learning simulation.\n",
    "\n",
    "    This callback manages a queue of positions that represent the agent's planned trajectory during cognitive navigation tasks. It updates the agent's next movement target based on this schedule.\n",
    "\n",
    "    Inherits:\n",
    "        MovementSchedulerCallback: For basic movement scheduling functionalities.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to manage and update the mental movement schedule and trajectory.\n",
    "    \"\"\"\n",
    "    def set_cache(self, cache: Any, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the mental movement schedule and trajectory based on planned positions, and prepares for movement scheduling.\n",
    "\n",
    "        Args:\n",
    "            cache (Any): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'mental_movement_params',\n",
    "            'mental_movement_schedule',\n",
    "            'mental_trajectory'\n",
    "        ]\n",
    "        cache['mental_movement_schedule'] = self.positions\n",
    "        cache['mental_trajectory'] = deepcopy(self.positions)\n",
    "        BaseCallback.set_cache(self, cache, on_repeat)\n",
    "\n",
    "    def on_step_end(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the mental movement target at the end of each simulation step, based on the current schedule of planned movements.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if len(self.mental_movement_schedule):\n",
    "            if self.mental_movement_params.move_target is None:\n",
    "                self.mental_movement_params.move_target = self.mental_movement_schedule.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentalTrajectoryCallback(TrajectoryCallback):\n",
    "    \"\"\"\n",
    "    A specialized callback for managing the mental trajectory of an agent in an agent-based learning simulation.\n",
    "\n",
    "    This callback updates the agent's mental trajectory and movement schedule based on its current position, movement target, and direction. It ensures that the mental trajectory reflects the planned path towards the target.\n",
    "\n",
    "    Inherits:\n",
    "        TrajectoryCallback: For basic trajectory functionalities.\n",
    "\n",
    "    Attributes:\n",
    "        Requires various parameters from the cache to manage and update the mental trajectory and movement schedule.\n",
    "    \"\"\"\n",
    "    def set_cache(self, cache: Any, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the mental trajectory if not present, and prepares for trajectory management.\n",
    "\n",
    "        Args:\n",
    "            cache (Any): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'mental_movement_params',\n",
    "            'mental_movement_schedule',\n",
    "            'mental_trajectory'\n",
    "        ]\n",
    "\n",
    "        if 'mental_trajectory' not in cache:\n",
    "            cache['mental_trajectory'] = list()\n",
    "\n",
    "        BaseCallback.set_cache(self, cache, on_repeat)\n",
    "\n",
    "    def on_step_begin(self, step: int):\n",
    "        \"\"\"\n",
    "        Updates the mental trajectory and movement schedule at the beginning of each simulation step, based on the agent's current position, movement target, and direction.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if self.mental_movement_params.move_target is not None:\n",
    "            if self.mental_movement_params.move_target not in self.mental_trajectory:\n",
    "                xy = self.trajectory_manager(\n",
    "                    self.mental_movement_params.position,\n",
    "                    self.mental_movement_params.move_target,\n",
    "                    self.mental_movement_params.direction\n",
    "                )\n",
    "                self.mental_trajectory.clear()\n",
    "                self.mental_trajectory += [tuple(item) for item in xy.tolist()]\n",
    "                self.mental_movement_schedule.clear()\n",
    "                self.mental_movement_schedule += deepcopy(self.mental_trajectory)\n",
    "                self.mental_movement_params.move_target = self.mental_movement_schedule.pop(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Hashable, Literal\n",
    "\n",
    "from bbtoolkit.structures import DotDict\n",
    "\n",
    "@dataclass\n",
    "class DynamicParameters(DotDict):\n",
    "    \"\"\"\n",
    "    A data class to hold dynamic parameters for the simulation, extending DotDict for dot-accessible dictionary attributes.\n",
    "\n",
    "    Attributes:\n",
    "        dt (float): The time step for the simulation.\n",
    "        mode (Literal['bottom-up', 'top-down', 'recall']): The current mode of the simulation.\n",
    "        step (int): The current step number in the simulation, defaulting to 0.\n",
    "    \"\"\"\n",
    "    dt: float\n",
    "    mode: Literal['bottom-up', 'top-down', 'recall']\n",
    "    step: int = field(default_factory=lambda:0)\n",
    "\n",
    "@dataclass\n",
    "class EcodingParameters(DotDict):\n",
    "    \"\"\"\n",
    "    A data class to hold encoding parameters for the simulation, including information about encoded objects and the object currently being recalled.\n",
    "\n",
    "    Attributes:\n",
    "        encoded_objects (DirectedTensorGroup): A group of tensors representing encoded objects, defaulting to None.\n",
    "        object_to_recall (int): The identifier of the object to recall, defaulting to None.\n",
    "    \"\"\"\n",
    "    encoded_objects: DirectedTensorGroup = field(default_factory=lambda:None)\n",
    "    object_to_recall: int = field(default_factory=lambda:None)\n",
    "\n",
    "@dataclass\n",
    "class ClickParameters(DotDict):\n",
    "    \"\"\"\n",
    "    A data class to hold parameters related to mouse click interactions within the simulation.\n",
    "\n",
    "    Attributes:\n",
    "        xy_data (tuple[float, float]): The x and y coordinates of the click, defaulting to None.\n",
    "        inside_object (bool): A flag indicating whether the click was inside an object, defaulting to False.\n",
    "        inside_wall (bool): A flag indicating whether the click was inside a wall, defaulting to False.\n",
    "    \"\"\"\n",
    "    xy_data: tuple[float, float] = field(default_factory=lambda:None)\n",
    "    inside_object: bool = field(default_factory=bool)\n",
    "    inside_wall: bool = field(default_factory=bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_train_x, max_train_x = -8, 0\n",
    "min_train_y, max_train_y = -8, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.arange(\n",
    "    min_train_x,\n",
    "    max_train_x,\n",
    "    res\n",
    ")), int((max_train_x - min_train_x)/res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_train_x, max_train_x = -8.8, 8.600000000000048\n",
    "min_train_y, max_train_y = -8.8, 8.600000000000048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 59)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.meshgrid(np.arange(\n",
    "    min_train_x,\n",
    "    max_train_x,\n",
    "    res\n",
    "), np.arange(\n",
    "    min_train_x,\n",
    "    max_train_x,\n",
    "    res\n",
    "))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid2CartTransition:\n",
    "    \"\"\"\n",
    "    A class to manage the transition between grid coordinates and Cartesian coordinates within a given environment.\n",
    "\n",
    "    This class facilitates the conversion of coordinates from a grid-based representation to a Cartesian (x, y) representation and vice versa, based on the environment's parameters.\n",
    "\n",
    "    Attributes:\n",
    "        env (Environment): The environment object containing parameters for coordinate conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: Environment):\n",
    "        \"\"\"\n",
    "        Initializes the Grid2CartTransition instance with environment parameters.\n",
    "\n",
    "        Args:\n",
    "            env (Environment): The environment object containing parameters for coordinate conversion.\n",
    "        \"\"\"\n",
    "        coords_x, coords_y = env.params.coords[:, 0], env.params.coords[:, 1]\n",
    "        self.min_train_x, self.max_train_x, self.min_train_y, self.max_train_y = min(coords_x), max(coords_x), min(coords_y), max(coords_y)\n",
    "        self.shape = int((self.max_train_x - self.min_train_x)/env.params.res), int((self.max_train_y - self.min_train_y)/env.params.res)\n",
    "\n",
    "\n",
    "    def __call__(self, x: float, y: float) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Converts Cartesian coordinates to grid indices.\n",
    "\n",
    "        Args:\n",
    "            x (float): The x-coordinate in Cartesian space.\n",
    "            y (float): The y-coordinate in Cartesian space.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: The corresponding grid indices.\n",
    "        \"\"\"\n",
    "        x = (x - self.min_train_x)/(self.max_train_x - self.min_train_x)\n",
    "        y = (y - self.min_train_y)/(self.max_train_y - self.min_train_y)\n",
    "\n",
    "        return self.shape[1] - int(y*self.shape[1]), int(x*self.shape[0])\n",
    "\n",
    "    def __getitem__(self, indices: tuple[int, int]) -> tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Converts grid indices to Cartesian coordinates.\n",
    "\n",
    "        Args:\n",
    "            indices (tuple[int, int]): The grid indices.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: The corresponding Cartesian coordinates.\n",
    "        \"\"\"\n",
    "        i, j = indices\n",
    "        x = ((self.shape[0] - i) / self.shape[0]) * (self.max_train_x - self.min_train_x) + self.min_train_x\n",
    "        # FIXME: y-axis is inverted\n",
    "        y = ((j) / self.shape[1]) * (self.max_train_y - self.min_train_y) + self.min_train_y\n",
    "        return y, x # because in a matrix first index goes for columns and second for rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from bbtoolkit.structures import DotDict\n",
    "\n",
    "@dataclass\n",
    "class BBCache(DotDict):\n",
    "    \"\"\"\n",
    "    A data class to hold the simulation cache for a brain-based controller (BBC), extending DotDict for dot-accessible dictionary attributes.\n",
    "\n",
    "    Attributes:\n",
    "        connectivity (DirectedTensorGroup): The connectivity information between different parts of the simulation.\n",
    "        weights (DirectedTensorGroup): The synaptic weights between neurons or groups of neurons.\n",
    "        k_ratio (TensorGroup): The ratio of different types of connections or activities.\n",
    "        activity (TensorGroup): The neural activity within the simulation.\n",
    "        rates (TensorGroup): The firing rates of neurons or groups of neurons.\n",
    "        tc_gen (TCGenerator): The transformation circuit generator for the simulation.\n",
    "        env (Environment): The environment in which the agent operates.\n",
    "        grid2cart (Grid2CartTransition): The transition matrix or function from grid to Cartesian coordinates.\n",
    "        dynamics_params (DynamicParameters): The dynamic parameters controlling the simulation's state and progression.\n",
    "        encoding_params (EcodingParameters): The encoding parameters related to object recognition and memory, defaulting to an empty instance.\n",
    "        click_params (ClickParameters): The parameters related to user interactions with the simulation, defaultally to an empty instance.\n",
    "    \"\"\"\n",
    "    connectivity: DirectedTensorGroup\n",
    "    weights: DirectedTensorGroup\n",
    "    k_ratio: TensorGroup\n",
    "    activity: TensorGroup\n",
    "    rates: TensorGroup\n",
    "    tc_gen: TCGenerator\n",
    "    env: Environment\n",
    "    grid2cart: Grid2CartTransition\n",
    "    dynamics_params: DynamicParameters\n",
    "    encoding_params: EcodingParameters = field(default_factory=lambda:EcodingParameters())\n",
    "    click_params: ClickParameters = field(default_factory=lambda:ClickParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Mapping\n",
    "\n",
    "\n",
    "class FramesStoringCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A callback designed to store frames of the simulation at a specified rate.\n",
    "\n",
    "    This callback saves snapshots of the simulation's current state, allowing for the creation of visualizations such as animations or time-lapse videos.\n",
    "\n",
    "    Attributes:\n",
    "        save_rate (int): The rate at which frames are saved (e.g., every `save_rate` steps).\n",
    "        savedir (str): The directory where the frames are saved.\n",
    "    \"\"\"\n",
    "    def __init__(self, save_rate: int, savedir: str):\n",
    "        \"\"\"\n",
    "        Initializes the FramesStoringCallback instance with a specified save rate and directory.\n",
    "\n",
    "        Args:\n",
    "            save_rate (int): The rate at which frames are saved.\n",
    "            savedir (str): The directory where the frames are saved.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_rate = save_rate\n",
    "        self.savedir = savedir\n",
    "\n",
    "    def set_cache(\n",
    "        self,\n",
    "        cache: Mapping,\n",
    "        on_repeat: str = 'raise'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping and specifies the required cache keys for saving frames.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = ['fig', 'dynamics_params']\n",
    "        super().set_cache(cache, on_repeat)\n",
    "\n",
    "    def on_step_end(self, step: int):\n",
    "        \"\"\"\n",
    "        Executes the logic for saving a frame at the end of a simulation step, based on the specified save rate.\n",
    "\n",
    "        Args:\n",
    "            step (int): The current step number.\n",
    "        \"\"\"\n",
    "        if self.dynamics_params.step % self.save_rate == 0:\n",
    "            self.fig.savefig(f'{self.savedir}/frame_{self.dynamics_params.step}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oPRPlotter(ArtistCallback):\n",
    "    \"\"\"\n",
    "    A specialized ArtistCallback for plotting the activations of perirhinal identity neurons (oPR) for objects in an agent-based learning simulation.\n",
    "\n",
    "    This callback visualizes the oPR neuron activations, providing insights into the agent's recognition and encoding of objects based on their identities.\n",
    "\n",
    "    Attributes:\n",
    "        color_new (str): The color used to indicate newly encountered objects. Defaults to 'tab:blue'.\n",
    "        color_enc (str): The color used to indicate previously encoded objects. Defaults to 'tab:red'.\n",
    "        labels (list): A list of labels for the objects, derived from their textures.\n",
    "    \"\"\"\n",
    "    def __init__(self, color_new: str = 'tab:blue', color_enc: str = 'tab:red'):\n",
    "        \"\"\"\n",
    "        Initializes the oPRPlotter instance with specified colors for newly encountered and previously encoded objects.\n",
    "\n",
    "        Args:\n",
    "            color_new (str, optional): The color for newly encountered objects. Defaults to 'tab:blue'.\n",
    "            color_enc (str, optional): The color for previously encoded objects. Defaults to 'tab:red'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.labels = []\n",
    "        self.color_enc = color_enc\n",
    "        self.color_new = color_new\n",
    "\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        \"\"\"\n",
    "        Sets the cache with the provided mapping, initializes the plotting axis for oPR representation, and specifies the required cache keys for plotting.\n",
    "\n",
    "        Args:\n",
    "            cache (Mapping): The new cache mapping.\n",
    "            on_repeat (str): The behavior when a cache key is already an attribute. Defaults to 'raise'.\n",
    "        \"\"\"\n",
    "        self.requires = [\n",
    "            'env',\n",
    "            'fig',\n",
    "            'gc',\n",
    "            'rates',\n",
    "            'encoding_params',\n",
    "            'opr_ax'\n",
    "        ]\n",
    "        cache['opr_ax'] = cache.fig.add_subplot(cache.gc[8:11, 4:8])\n",
    "        super().set_cache(cache, on_repeat=on_repeat)\n",
    "\n",
    "        for obj in self.env.objects:\n",
    "            self.labels.append(\n",
    "                obj.polygon.texture.name\n",
    "            )\n",
    "\n",
    "    def plot(self):\n",
    "        encoded_indices = np.where(self.encoding_params.encoded_objects.ovc.to.h)[0]\n",
    "\n",
    "        for i, opr_rate in enumerate(self.rates.opr):\n",
    "\n",
    "            rect = plt.Rectangle(\n",
    "                (i-.4, 0),\n",
    "                .8,\n",
    "                1.1,\n",
    "                color=self.color_enc if i in encoded_indices else self.color_new,\n",
    "                alpha=0.3,\n",
    "                linewidth=0\n",
    "            )\n",
    "            self.opr_ax.add_patch(rect)\n",
    "\n",
    "            rect_border = plt.Rectangle(\n",
    "                (i - 0.2, 0),\n",
    "                0.4, opr_rate[0],\n",
    "                fill=False,\n",
    "                edgecolor='tab:grey',\n",
    "                linewidth=1,\n",
    "                linestyle='--',\n",
    "                hatch='\\\\\\\\\\\\' if i in encoded_indices else '///'\n",
    "            )\n",
    "            self.opr_ax.add_patch(rect_border)\n",
    "\n",
    "    def on_plot(self):\n",
    "        \"\"\"\n",
    "        Executes the plotting logic for the oPR neuron activations, using different visual cues for newly encountered and previously encoded objects.\n",
    "        \"\"\"\n",
    "        self.plot()\n",
    "\n",
    "\n",
    "    def on_clean(self):\n",
    "        \"\"\"\n",
    "        Clears the plot in preparation for the next update and sets up the axis for better readability and object identification.\n",
    "        \"\"\"\n",
    "        self.opr_ax.clear()\n",
    "        self.opr_ax.set_ylim(0, 1)\n",
    "        self.opr_ax.set_xlim(-.5, len(self.rates.opr)-.5)\n",
    "        self.opr_ax.set_xticks(range(len(self.labels)), self.labels, rotation=45, ha='right')\n",
    "        self.opr_ax.set_yticks([])\n",
    "        for spine_name, spine in self.opr_ax.spines.items():\n",
    "            if spine_name in ('bottom', 'top'):\n",
    "                spine.set_edgecolor('tab:grey')\n",
    "            elif spine_name in ('left', 'right'):\n",
    "                spine.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAGHRFWHRUaXRsZQBmcm9tX2xpc3QgY29sb3JtYXBDL89FAAAAHnRFWHREZXNjcmlwdGlvbgBmcm9tX2xpc3QgY29sb3JtYXABtCnOAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My44LjQsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmdkzrK0AAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ0poLZMAAAFPSURBVHic7daxDQIxAATBe/qvhgZNAilkCLEzkmUHFzjca/f72bbrda7r89vuv3e/8Ae7r+y2beds5+w873dvu//e/cIf7L6023YbAJAjAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAEPQAsQkpPiMR9AQAAAABJRU5ErkJggg==",
      "text/html": [
       "<div style=\"vertical-align: middle;\"><strong>from_list</strong> </div><div class=\"cmap\"><img alt=\"from_list colormap\" title=\"from_list\" style=\"border: 1px solid #555;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAgAAAABACAYAAABsv8+/AAAAGHRFWHRUaXRsZQBmcm9tX2xpc3QgY29sb3JtYXBDL89FAAAAHnRFWHREZXNjcmlwdGlvbgBmcm9tX2xpc3QgY29sb3JtYXABtCnOAAAAMHRFWHRBdXRob3IATWF0cGxvdGxpYiB2My44LjQsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmdkzrK0AAAAMnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHYzLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZ0poLZMAAAFPSURBVHic7daxDQIxAATBe/qvhgZNAilkCLEzkmUHFzjca/f72bbrda7r89vuv3e/8Ae7r+y2beds5+w873dvu//e/cIf7L6023YbAJAjAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAECQAACBIAABAkAAAgSAAAQJAAAIAgAQAAQQIAAIIEAAAEPQAsQkpPiMR9AQAAAABJRU5ErkJggg==\"></div><div style=\"vertical-align: middle; max-width: 514px; display: flex; justify-content: space-between;\"><div style=\"float: left;\"><div title=\"#00b2b2ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00b2b2ff;\"></div> under</div><div style=\"margin: 0 auto; display: inline-block;\">bad <div title=\"#00000000\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #00000000;\"></div></div><div style=\"float: right;\">over <div title=\"#b20000ff\" style=\"display: inline-block; width: 1em; height: 1em; margin: 0; vertical-align: middle; border: 1px solid #555; background-color: #b20000ff;\"></div></div>"
      ],
      "text/plain": [
       "<matplotlib.colors.ListedColormap at 0x73d7c7ec1bb0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bbtoolkit.utils.viz.colors import generate_cmap\n",
    "\n",
    "red = '#b20000'\n",
    "green = '#00b2b2'\n",
    "cmap = generate_cmap(*[\n",
    "    # '#003535',\n",
    "    '#00b2b2',\n",
    "    '#fff',\n",
    "    '#b20000'\n",
    "])\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugArtist(ArtistCallback):\n",
    "    def set_cache(self, cache: Mapping, on_repeat: str = 'raise'):\n",
    "        self.requires = [\n",
    "            'fig',\n",
    "            'gc',\n",
    "            'rates',\n",
    "            'activity',\n",
    "            'debug_ax1',\n",
    "            'debug_ax1_1',\n",
    "            'weights',\n",
    "            'grid2cart',\n",
    "            'movement_params',\n",
    "            'encoding_params',\n",
    "        ]\n",
    "        cache['debug_ax1'] = cache['fig'].add_subplot(cache['gc'][8:10, 4:6])\n",
    "        cache['debug_ax1_1'] = cache['fig'].add_subplot(cache['gc'][8:10, 6:8])\n",
    "        cache['fig'].tight_layout()\n",
    "        super().set_cache(cache, on_repeat=on_repeat)\n",
    "\n",
    "    def on_plot(self):\n",
    "        # self.debug_ax1.imshow(self.weights.ovc.to.opr, aspect='auto', origin='lower')\n",
    "        self.debug_ax1.imshow(\n",
    "            np.reshape(\n",
    "                (self.weights.gc.to.h@self.rates.gc),\n",
    "                self.grid2cart.shape\n",
    "            ),\n",
    "            cmap='coolwarm',\n",
    "        )\n",
    "        # self.debug_ax1.plot(self.activity.opr, 'o')\n",
    "        temp = np.zeros(self.grid2cart.shape)\n",
    "        temp[*self.grid2cart(*self.movement_params.position)] = 1\n",
    "        self.debug_ax1_1.imshow(temp, cmap='coolwarm')\n",
    "\n",
    "    def on_clean(self):\n",
    "        self.debug_ax1.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:12:25 - DEBUG - Loaded backend QtAgg version 6.7.0.\n",
      "QApplication: invalid style override 'kvantum' passed, ignoring it.\n",
      "\tAvailable styles: Windows, Fusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:12:27 - DEBUG - HD CUE INITIATED\n",
      "2024-05-19 16:12:44 - DEBUG - HD CUE REMOVED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:16:00 - DEBUG - Switch attention to 2\n",
      "2024-05-19 16:16:12 - DEBUG - OVC2H FOR OBJECT 2 UPDATED\n",
      "2024-05-19 16:16:12 - DEBUG - OVC2OPR FOR OBJECT 2 UPDATED\n",
      "2024-05-19 16:16:12 - DEBUG - H2OPR FOR OBJECT 2 UPDATED\n",
      "2024-05-19 16:16:12 - DEBUG - HD2OPR FOR OBJECT 2 UPDATED\n",
      "2024-05-19 16:16:12 - DEBUG - BVC2OVC FOR OBJECT 2 UPDATED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovc h 1.0 1.0\n",
      "ovc opr 1.0 1.0\n",
      "h opr 1.0 1.0\n",
      "bvc ovc 1.0 1.0\n",
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:17:13 - DEBUG - Switch attention to 4\n",
      "2024-05-19 16:17:24 - DEBUG - OVC2H FOR OBJECT 4 UPDATED\n",
      "2024-05-19 16:17:24 - DEBUG - OVC2OPR FOR OBJECT 4 UPDATED\n",
      "2024-05-19 16:17:24 - DEBUG - H2OPR FOR OBJECT 4 UPDATED\n",
      "2024-05-19 16:17:24 - DEBUG - HD2OPR FOR OBJECT 4 UPDATED\n",
      "2024-05-19 16:17:24 - DEBUG - BVC2OVC FOR OBJECT 4 UPDATED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovc h 1.0 1.0\n",
      "ovc opr 1.0 1.0\n",
      "h opr 1.0 1.0\n",
      "bvc ovc 1.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:17:30 - DEBUG - Switch attention to 2\n",
      "2024-05-19 16:17:47 - DEBUG - Switch attention to 4\n",
      "2024-05-19 16:18:03 - DEBUG - Switch attention to 2\n",
      "2024-05-19 16:18:19 - DEBUG - Switch attention to 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:18:52 - DEBUG - Switch attention to 2\n",
      "2024-05-19 16:19:08 - DEBUG - Switch attention to 4\n",
      "2024-05-19 16:19:25 - DEBUG - Switch attention to 2\n",
      "2024-05-19 16:19:42 - DEBUG - Switch attention to 4\n",
      "2024-05-19 16:20:15 - DEBUG - Initiate recall for object 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 16:20:53 - DEBUG - Switching to top-down mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n",
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 127\u001b[0m\n\u001b[1;32m     32\u001b[0m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbvc\u001b[38;5;241m.\u001b[39mto\u001b[38;5;241m.\u001b[39mtr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mlen\u001b[39m(weights\u001b[38;5;241m.\u001b[39mtr\u001b[38;5;241m.\u001b[39mto\u001b[38;5;241m.\u001b[39mbvc))\n\u001b[1;32m     34\u001b[0m dynamics \u001b[38;5;241m=\u001b[39m DynamicsManager(\n\u001b[1;32m     35\u001b[0m     dt,\n\u001b[1;32m     36\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache\n\u001b[1;32m    124\u001b[0m )\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/__init__.py:96\u001b[0m, in \u001b[0;36mDynamicsManager.__call__\u001b[0;34m(self, time)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(time, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m time:\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_cycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(time, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[1;32m     98\u001b[0m     rest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/__init__.py:75\u001b[0m, in \u001b[0;36mDynamicsManager.run\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_iteration_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, n_steps)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_iteration_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/__init__.py:55\u001b[0m, in \u001b[0;36mDynamicsManager._step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle: \u001b[38;5;66;03m# only if new cycle is started\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_cycle_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mon_step_begin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_cycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_step_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/structures/__init__.py:155\u001b[0m, in \u001b[0;36mCallbacksCollection.execute\u001b[0;34m(self, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Executes a specified method on all callback objects in the collection.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        tuple: A tuple containing the results of executing the method on each callback object.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "Cell \u001b[0;32mIn[17], line 66\u001b[0m, in \u001b[0;36mPWCallback.on_step_begin\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_ratio\u001b[38;5;241m.\u001b[39mpw \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_cue_scale\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwalls_pw, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop-down\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_ratio\u001b[38;5;241m.\u001b[39mpw \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectivity\u001b[38;5;241m.\u001b[39mtr\u001b[38;5;241m.\u001b[39mto\u001b[38;5;241m.\u001b[39mpw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivity\u001b[38;5;241m.\u001b[39mpw \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectivity\u001b[38;5;241m.\u001b[39mpw\u001b[38;5;241m.\u001b[39mto\u001b[38;5;241m.\u001b[39mpw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_ratio\u001b[38;5;241m.\u001b[39mpw\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrates\u001b[38;5;241m.\u001b[39mpw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectivity\u001b[38;5;241m.\u001b[39mpw\u001b[38;5;241m.\u001b[39mto\u001b[38;5;241m.\u001b[39mpw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivity\u001b[38;5;241m.\u001b[39mpw \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnectivity\u001b[38;5;241m.\u001b[39mpw\u001b[38;5;241m.\u001b[39mto\u001b[38;5;241m.\u001b[39mpw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2172\u001b[0m, in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2167\u001b[0m \n\u001b[1;32m   2168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2173\u001b[0m                     initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2179\u001b[0m         initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "dt = .005\n",
    "# position = (-4, 7.5)\n",
    "# position = (0, 0)\n",
    "# position = (0, 0)\n",
    "# position = (5, -5)\n",
    "position = (7, -7)\n",
    "direction = np.pi/2\n",
    "# direction = -3*np.pi/4\n",
    "attn_dist = 7.5\n",
    "\n",
    "cache = BBCache(\n",
    "    connectivity.copy(),\n",
    "    weights.copy(),\n",
    "    k_ratio.copy(),\n",
    "    activity.copy(),\n",
    "    rates.copy(),\n",
    "    tc_gen,\n",
    "    compiler.environment,\n",
    "    Grid2CartTransition(compiler.environment),\n",
    "    DynamicParameters(dt, 'bottom-up'),\n",
    "    EcodingParameters(),\n",
    "    ClickParameters()\n",
    ")\n",
    "\n",
    "cache['weights'].tr.to.bvc = np.eye(len(weights.tr.to.bvc))\n",
    "cache['weights'].bvc.to.tr = np.eye(len(weights.tr.to.bvc))\n",
    "\n",
    "dynamics = DynamicsManager(\n",
    "    dt,\n",
    "    callbacks=[\n",
    "        TimerCallback(),\n",
    "        MovementCallback(\n",
    "            MovementManager(\n",
    "                5,\n",
    "                math.pi*2,\n",
    "                position,\n",
    "                direction\n",
    "            )\n",
    "        ),\n",
    "        MentalPositionCallback(),\n",
    "        MentalMovementCallback(\n",
    "            MovementManager(\n",
    "                5,\n",
    "                math.pi*2,\n",
    "                position,\n",
    "                direction\n",
    "            )\n",
    "        ),\n",
    "        FOVCallback(fov_manager),\n",
    "        EgoCallback(ego_manager),\n",
    "        EgoSegmentationCallback(),\n",
    "        ParietalWindowCallback(),\n",
    "        MovementSchedulerCallback(),\n",
    "        TrajectoryCallback(\n",
    "            AStarTrajectory(compiler.environment)\n",
    "        ),\n",
    "        MentalMovementSchedulerCallback(),\n",
    "        MentalTrajectoryCallback(\n",
    "            AStarTrajectory(compiler.environment)\n",
    "        ),\n",
    "        AttentionCallback(\n",
    "            DistanceAttention(7, dt, len(compiler.environment.objects), attn_dist)\n",
    "        ),\n",
    "        HDCallback(),\n",
    "        GCRateCallback(gc_map),\n",
    "        PCCallback(),\n",
    "        BVCCallback(),\n",
    "        OVCCallback(),\n",
    "        PRCallback(),\n",
    "        oPRCallback(),\n",
    "        PWCallback(),\n",
    "        oPWCallback(),\n",
    "        IPRateCallback(),\n",
    "        TCCallback(),\n",
    "        oTCCallback(),\n",
    "        ObjectWeightsUpdatingCallback(),\n",
    "        PlottingCallback(\n",
    "            [\n",
    "                AloEnvPlotter(\n",
    "                    attn_color=red\n",
    "                ),\n",
    "                EgoEnvPlotter(\n",
    "                    attn_color=red\n",
    "                ),\n",
    "                MouseEventCallback(),\n",
    "                TimerPlotter(),\n",
    "                AgentPlotter(),\n",
    "                TargetPlotter(\n",
    "                    move_target_color=red\n",
    "                ),\n",
    "                TrajectoryPlotter(\n",
    "                    target_color=red\n",
    "                ),\n",
    "                PWPlotter(cmap=cmap),\n",
    "                BVCPlotter(cmap=cmap),\n",
    "                oPWPlotter(cmap=cmap),\n",
    "                OVCPlotter(cmap=cmap),\n",
    "                HDPlotter(cmap=cmap),\n",
    "                PCPlotter(cmap=cmap),\n",
    "                oPRPlotter(\n",
    "                    '#00b2b2',\n",
    "                    '#b20000'\n",
    "                ),\n",
    "                ObjectRecallCallback(),\n",
    "                PickedObjectPlotter(),\n",
    "                DistanceAttentionPlotter(attn_dist, color=red),\n",
    "                MentalAgentPlotter(),\n",
    "                MentalTargetPlotter(),\n",
    "                MentalTrajectoryPlotter()\n",
    "            ],\n",
    "            update_rate=5,\n",
    "            fig_kwargs=dict(figsize=(10, 10)),\n",
    "            gc_kwargs=dict(nrows=12, ncols=12)\n",
    "        ),\n",
    "        FramesStoringCallback(5, '../tmp')\n",
    "    ],\n",
    "    cache=cache\n",
    ")\n",
    "\n",
    "\n",
    "for _ in dynamics(True):\n",
    "    print('out: ', _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most visible color on the 'viridis' colormap is: #fdfefe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_most_visible_color(cmap):\n",
    "    \"\"\"\n",
    "    Returns the hex code of the most visible color on the given colormap\n",
    "    when viewed on a grayscale background.\n",
    "\n",
    "    Args:\n",
    "        cmap (str or Colormap): The colormap to analyze.\n",
    "\n",
    "    Returns:\n",
    "        str: The hex code of the most visible color.\n",
    "    \"\"\"\n",
    "    # Ensure cmap is a Colormap instance\n",
    "    if isinstance(cmap, str):\n",
    "        cmap = plt.get_cmap(cmap)\n",
    "\n",
    "    # Generate an array of points along the colormap\n",
    "    colors = cmap(np.linspace(0, 1, 256))\n",
    "\n",
    "    # Convert colors to grayscale using the luminosity method\n",
    "    # This method better represents human perception\n",
    "    grayscale = 0.21 * colors[:, 0] + 0.72 * colors[:, 1] + 0.07 * colors[:, 2]\n",
    "\n",
    "    # Find the color with the maximum contrast to the midpoint of the grayscale spectrum\n",
    "    # The midpoint is 0.5 in normalized grayscale (0 is black, 1 is white)\n",
    "    contrast = np.abs(grayscale - 0.5)\n",
    "    max_contrast_index = np.argmax(contrast)\n",
    "\n",
    "    # Convert the most visible color to hex format\n",
    "    most_visible_color = colors[max_contrast_index]\n",
    "    hex_color = matplotlib.colors.rgb2hex(most_visible_color[:3])\n",
    "\n",
    "    return hex_color\n",
    "\n",
    "# Example usage:\n",
    "cmap_name = 'viridis'  # Change this to any colormap name you like\n",
    "most_visible_color = get_most_visible_color(cmap)\n",
    "print(f\"The most visible color on the '{cmap_name}' colormap is: {most_visible_color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "%matplotlib inline\n",
    "ax = fig.add_subplot(111, projection='polar')\n",
    "ax.grid(color='red')\n",
    "# ax.contourf(\n",
    "#             self.theta_bvc.T,\n",
    "#             self.r_bvc.T,\n",
    "#             np.reshape(np.maximum(self.rates.ovc, 1e-7), (self.tc_gen.n_bvc_theta, self.tc_gen.n_bvc_r)),\n",
    "#             cmap=self.cmap,\n",
    "#             vmin=0, vmax=1\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.weights.ovc.to.opr.max(), cache.weights.opr.to.ovc.max(), cache.weights['opr'].to['ovc'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.weights['hd'].to['hd'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'threshold': 0.05, 'update_threshold': 0.05}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ObjectWeightsUpdatingCallback().population_thresholds.opr.to.ovc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[403], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vis \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mrates\u001b[38;5;241m.\u001b[39mhd\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      2\u001b[0m th_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(vis \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m.1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m breakdown \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mth_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m breakdown\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m      5\u001b[0m     th_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([th_indices[breakdown\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:], th_indices[:breakdown\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], ])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "vis = cache.rates.hd.copy()\n",
    "th_indices = np.where(vis > .1)[0]\n",
    "breakdown = np.where(np.diff(th_indices) != 1)[0]\n",
    "if breakdown.size:\n",
    "    th_indices = np.concatenate([th_indices[breakdown[0]+1:], th_indices[:breakdown[0]+1], ])\n",
    "\n",
    "for i in range(len(th_indices)//2):\n",
    "    this_index = ((i)+th_indices[len(th_indices)//2])%len(vis)\n",
    "    next_index = (this_index + 1)%len(vis)\n",
    "    temp = vis[next_index]\n",
    "    vis[next_index] = vis[this_index]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([95, 96, 97, 98, 99,  0,  1,  2])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.diff(np.where(np.array([1, 2]) > .1)[0]) != 1)[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99108521])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.weights.bvc.to.ovc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.weights.ovc.to.bvc = cache.weights.bvc.to.ovc.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.weights.opr.to.ovc = cache.weights.ovc.to.opr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 19:48:28 - DEBUG - Stop recall\n",
      "2024-05-18 19:48:29 - DEBUG - HD CUE INITIATED\n",
      "2024-05-18 19:48:46 - DEBUG - HD CUE REMOVED\n",
      "2024-05-18 19:49:03 - DEBUG - Initiate recall for object 2\n",
      "2024-05-18 19:49:23 - DEBUG - Stop recall\n",
      "2024-05-18 19:49:23 - DEBUG - HD CUE INITIATED\n",
      "2024-05-18 19:49:40 - DEBUG - Initiate recall for object 2\n",
      "2024-05-18 19:49:40 - DEBUG - HD CUE REMOVED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  (None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/__init__.py:96\u001b[0m, in \u001b[0;36mDynamicsManager.__call__\u001b[0;34m(self, time)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(time, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m time:\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_cycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(time, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[1;32m     98\u001b[0m     rest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/__init__.py:75\u001b[0m, in \u001b[0;36mDynamicsManager.run\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_iteration_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, n_steps)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_iteration_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/__init__.py:55\u001b[0m, in \u001b[0;36mDynamicsManager._step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle: \u001b[38;5;66;03m# only if new cycle is started\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_cycle_begin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mon_step_begin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_cycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_step_end\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimer\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_cycle)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/structures/__init__.py:155\u001b[0m, in \u001b[0;36mCallbacksCollection.execute\u001b[0;34m(self, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Executes a specified method on all callback objects in the collection.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        tuple: A tuple containing the results of executing the method on each callback object.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/callbacks/fov.py:131\u001b[0m, in \u001b[0;36mEgoCallback.on_step_begin\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mUpdates the agent's ego-centric representation at the beginning of each simulation step based on the current position and direction.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    step (int): The current step of the simulation.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovement_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdirection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     walls_ego, objects_ego \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mego\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovement_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovement_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, wall \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(walls_ego):\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwalls_ego\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m wall\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/environment/fov/ego.py:70\u001b[0m, in \u001b[0;36mEgoManager.__call__\u001b[0;34m(self, position, direction)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mApplies ego-centric transformations to the field of view data based on the agent's position and direction.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m        and the second list contains the ego-centric coordinates of objects.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m direction \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m---> 70\u001b[0m walls_fov, objects_fov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m walls_ego \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative(walls_fov, position), direction \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     72\u001b[0m objects_ego \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative(objects_fov, position), direction \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/environment/fov/__init__.py:52\u001b[0m, in \u001b[0;36mFOVManager.__call__\u001b[0;34m(self, position, direction)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mCalculates and returns the visible walls and objects within the FOV from the specified position and direction.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m        - The second list contains the visible parts of objects within the FOV.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m phi1, phi2 \u001b[38;5;241m=\u001b[39m get_fov(direction, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfov)\n\u001b[1;32m     51\u001b[0m objects_fov \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisible_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m     53\u001b[0m         points_within_angles(\n\u001b[1;32m     54\u001b[0m             obj\u001b[38;5;241m.\u001b[39mvisible_parts(\u001b[38;5;241m*\u001b[39mposition) \u001b[38;5;241m-\u001b[39m position,\n\u001b[1;32m     55\u001b[0m             phi1, phi2\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m     ]\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mobjects\n\u001b[1;32m     59\u001b[0m ]\n\u001b[1;32m     60\u001b[0m walls_fov \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     61\u001b[0m         wall\u001b[38;5;241m.\u001b[39mvisible_parts(\u001b[38;5;241m*\u001b[39mposition)[\n\u001b[1;32m     62\u001b[0m             points_within_angles(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m wall \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mwalls\n\u001b[1;32m     68\u001b[0m     ]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m walls_fov, objects_fov\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/environment/visible_planes.py:154\u001b[0m, in \u001b[0;36mVisiblePlaneSubset.__call__\u001b[0;34m(self, coords_x, coords_y)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03mMakes the VisiblePlaneSubset object callable.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    np.ndarray: The visible coordinates for the object.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# still efficient since LazyVisiblePlane.__call__ is cached\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisible_plane\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords_y\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_index]\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/environment/visible_planes.py:737\u001b[0m, in \u001b[0;36mLazyVisiblePlaneWithTransparancy.__call__\u001b[0;34m(self, coords_x, coords_y)\u001b[0m\n\u001b[1;32m    729\u001b[0m hard_visible_boundary_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m    730\u001b[0m     np\u001b[38;5;241m.\u001b[39mconcatenate(hard_plane),\n\u001b[1;32m    731\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboundary_points), \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    732\u001b[0m ], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transparent_slice, transparent_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransparent_vectors_slices,\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransparent_indices\n\u001b[1;32m    736\u001b[0m ):\n\u001b[0;32m--> 737\u001b[0m     hard_plane[transparent_index] \u001b[38;5;241m=\u001b[39m \u001b[43mnested_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarting_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransparent_slice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirections\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransparent_slice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_visible_boundary_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtransparent_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hard_plane\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/utils/data/__init__.py:172\u001b[0m, in \u001b[0;36mCached.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Wrapper function that adds caching functionality to the decorated function.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m        Any: The result of the decorated function.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencrypt_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Check if the result is already in the cache\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache:\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;66;03m# Move the key to the end to mark it as the most recently used\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/utils/data/__init__.py:157\u001b[0m, in \u001b[0;36mCached.encrypt_key\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03mHelper method to create a unique hash for the parameters.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    str: The MD5 hash of the concatenated string representation of args and kwargs.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m all_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 157\u001b[0m key \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39mmd5(\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mencode())\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m key\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:1508\u001b[0m, in \u001b[0;36m_array_repr_implementation\u001b[0;34m(arr, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1506\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(arr\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m0\u001b[39m,):\n\u001b[0;32m-> 1508\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[43marray2string\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_line_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress_small\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# show zero-length shape unless it is (0,)\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m     lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[], shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape),)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:736\u001b[0m, in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array2string\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:513\u001b[0m, in \u001b[0;36m_recursive_guard.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m repr_running\u001b[38;5;241m.\u001b[39madd(key)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m     repr_running\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:539\u001b[0m, in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    536\u001b[0m     summary_insert \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# find the right formatting function for the array\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m format_function \u001b[38;5;241m=\u001b[39m \u001b[43m_get_format_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# skip over \"[\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m next_line_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:472\u001b[0m, in \u001b[0;36m_get_format_function\u001b[0;34m(data, **options)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m formatdict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongfloat\u001b[39m\u001b[38;5;124m'\u001b[39m]()\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 472\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatdict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtypeobj, _nt\u001b[38;5;241m.\u001b[39mcomplexfloating):\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(dtypeobj, _nt\u001b[38;5;241m.\u001b[39mclongfloat):\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:411\u001b[0m, in \u001b[0;36m_get_formatdict.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_formatdict\u001b[39m(data, \u001b[38;5;241m*\u001b[39m, precision, floatmode, suppress, sign, legacy,\n\u001b[1;32m    404\u001b[0m                     formatter, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# note: extra arguments in kwargs are ignored\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# wrapped in lambdas to avoid taking a code path with the wrong type of data\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     formatdict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: BoolFormat(data),\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: IntegerFormat(data),\n\u001b[0;32m--> 411\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mFloatingFormat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloatmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongfloat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: FloatingFormat(\n\u001b[1;32m    414\u001b[0m             data, precision, floatmode, suppress, sign, legacy\u001b[38;5;241m=\u001b[39mlegacy),\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplexfloat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: ComplexFloatingFormat(\n\u001b[1;32m    416\u001b[0m             data, precision, floatmode, suppress, sign, legacy\u001b[38;5;241m=\u001b[39mlegacy),\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongcomplexfloat\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: ComplexFloatingFormat(\n\u001b[1;32m    418\u001b[0m             data, precision, floatmode, suppress, sign, legacy\u001b[38;5;241m=\u001b[39mlegacy),\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: DatetimeFormat(data, legacy\u001b[38;5;241m=\u001b[39mlegacy),\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimedelta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: TimedeltaFormat(data),\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: _object_format,\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: str_format,\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpystr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: repr_format}\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# we need to wrap values in `formatter` in a lambda, so that the interface\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# is the same as the above values.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindirect\u001b[39m(x):\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:932\u001b[0m, in \u001b[0;36mFloatingFormat.__init__\u001b[0;34m(self, data, precision, floatmode, suppress_small, sign, legacy)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlarge_exponent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:963\u001b[0m, in \u001b[0;36mFloatingFormat.fillFormat\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    959\u001b[0m     trim, unique \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    960\u001b[0m strs \u001b[38;5;241m=\u001b[39m (dragon4_scientific(x, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision,\n\u001b[1;32m    961\u001b[0m                    unique\u001b[38;5;241m=\u001b[39munique, trim\u001b[38;5;241m=\u001b[39mtrim, sign\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msign \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    962\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m finite_vals)\n\u001b[0;32m--> 963\u001b[0m frac_strs, _, exp_strs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m int_part, frac_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(s\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m frac_strs))\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m exp_strs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:963\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    959\u001b[0m     trim, unique \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    960\u001b[0m strs \u001b[38;5;241m=\u001b[39m (dragon4_scientific(x, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision,\n\u001b[1;32m    961\u001b[0m                    unique\u001b[38;5;241m=\u001b[39munique, trim\u001b[38;5;241m=\u001b[39mtrim, sign\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msign \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    962\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m finite_vals)\n\u001b[0;32m--> 963\u001b[0m frac_strs, _, exp_strs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43me\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    964\u001b[0m int_part, frac_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(s\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m frac_strs))\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m exp_strs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/venv/lib/python3.12/site-packages/numpy/core/arrayprint.py:960\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloatmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfixed\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_legacy \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m113\u001b[39m:\n\u001b[1;32m    959\u001b[0m     trim, unique \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 960\u001b[0m strs \u001b[38;5;241m=\u001b[39m (\u001b[43mdragon4_scientific\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m                   \u001b[49m\u001b[43munique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m finite_vals)\n\u001b[1;32m    963\u001b[0m frac_strs, _, exp_strs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(s\u001b[38;5;241m.\u001b[39mpartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strs))\n\u001b[1;32m    964\u001b[0m int_part, frac_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(s\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m frac_strs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in dynamics(True):\n",
    "    print('out: ', _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 23:14:43 - INFO - Video saved to ./full_experiment.mp4\n"
     ]
    }
   ],
   "source": [
    "from bbtoolkit.preprocessing.environment.viz import make_video_from_images\n",
    "\n",
    "\n",
    "image_directory = '../tmp'\n",
    "video_save_path = './full_experiment.mp4'\n",
    "make_video_from_images(image_directory, video_save_path, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_5.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_10.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_15.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_20.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_25.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_30.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_35.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_40.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_45.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_50.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_55.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_60.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_65.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_70.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_75.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_80.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_85.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_90.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_95.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_100.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_105.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_110.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_115.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_120.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_125.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_130.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_135.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_140.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_145.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_150.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_155.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_160.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_165.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_170.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_175.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_180.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_185.png\n",
      "2024-05-15 22:06:29 - INFO - Removed: ../tmp/frame_190.png\n"
     ]
    }
   ],
   "source": [
    "from bbtoolkit.data import remove_files_from_dir\n",
    "\n",
    "image_directory = '../tmp'\n",
    "remove_files_from_dir(image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics.save('../data/dynamics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DynamicsManager' object has no attribute 'attention_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdynamics_pretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_params\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DynamicsManager' object has no attribute 'attention_params'"
     ]
    }
   ],
   "source": [
    "dynamics_pretrained.attention_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m dynamics_pretrained \u001b[38;5;241m=\u001b[39m \u001b[43mDynamicsManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/dynamics.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m dynamics_pretrained(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout: \u001b[39m\u001b[38;5;124m'\u001b[39m, _)\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/dynamics/__init__.py:136\u001b[0m, in \u001b[0;36mDynamicsManager.load\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Loads a serialized DynamicsManager instance from a file.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        DynamicsManager: The loaded DynamicsManager instance.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43mWritablePickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     loaded\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_cache\u001b[39m\u001b[38;5;124m'\u001b[39m, loaded\u001b[38;5;241m.\u001b[39mcache, on_repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m     loaded\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mvalidate()\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/data/__init__.py:117\u001b[0m, in \u001b[0;36mWritablePickle.load\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    Load an object from a specified file path using pickle deserialization.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m        object: The loaded object.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pkl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SpatialMemory/bbtoolkit/data/__init__.py:29\u001b[0m, in \u001b[0;36mread_pkl\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mRead and deserialize an object from a pickle file.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m     26\u001b[0m     path,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 29\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "dynamics_pretrained = DynamicsManager.load('../data/dynamics.pkl')\n",
    "for _ in dynamics_pretrained(True):\n",
    "    print('out: ', _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionParams(attend_to=3, attention_priority=array([0., 0., 0., 0., 0.]), attention_step=21, attention_cycle=28)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamics_pretrained.cache.attention_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamics_pretrained.cache.objects_pw[dynamics_pretrained.cache.attention_params['attend_to']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dynamics_pretrained.cache.objects_pw)):\n",
    "    dynamics_pretrained.cache.objects_pw[i] = dynamics.cache.objects_pw[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
