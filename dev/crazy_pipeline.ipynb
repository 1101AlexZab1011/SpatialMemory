{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Feature Vector: [-0.7450707  -0.6669855   0.9939027  -0.11026069 -0.58076668  0.81407006\n",
      " -0.21917679 -0.97568516  0.87314217  0.48746565]\n"
     ]
    }
   ],
   "source": [
    "def expand_angle_to_vector(phi: float, n: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands an angular input phi (in radians) into a feature vector of size n.\n",
    "    The expansion includes sine and cosine terms for multiple frequencies,\n",
    "    effectively creating a Fourier series-like representation.\n",
    "\n",
    "    Args:\n",
    "        phi (float): the angle in radians.\n",
    "        n (int): the size of the output feature vector. Should be even.\n",
    "\n",
    "    Returns:\n",
    "        numpy array, the expanded feature vector for the angle.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if n is not an even number.\n",
    "\n",
    "    Notes:\n",
    "        - As i increases, the model becomes more sensitive to smaller changes in phi.\n",
    "            This is because a small change in phi will result in a larger change in sin((i + 1) * phi) and cos((i + 1) * phi)\n",
    "            for higher values of i. This can help the model to detect and respond to finer nuances in the data related to orientation.\n",
    "        - The combination of sine and cosine functions at different frequencies provides\n",
    "            a diverse set of features that can help the model to disentangle and learn complex patterns related to angular orientation.\n",
    "        - if i were used directly without adding 1, the first pair of sine and cosine ((i = 0)) would always be sin(0) = 0 and cos(0) = 1,\n",
    "            which does not provide meaningful information about (\\phi). By starting with ((i + 1)),\n",
    "            we ensure that the first pair effectively encodes the original angle, and subsequent pairs encode its harmonics.\n",
    "    \"\"\"\n",
    "    # Ensure n is even for balanced sine and cosine features\n",
    "    if n % 2 != 0:\n",
    "        raise ValueError(\"n must be an even number.\")\n",
    "\n",
    "    # Initialize the feature vector\n",
    "    feature_vector = np.zeros(n)\n",
    "\n",
    "    # Fill the feature vector with sine and cosine terms\n",
    "    for i in range(n // 2):\n",
    "        feature_vector[2*i] = np.sin((i + 1) * phi)\n",
    "        feature_vector[2*i + 1] = np.cos((i + 1) * phi)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "# Example usage\n",
    "phi = np.pi / 4  # 45 degrees in radians\n",
    "phi = np.random.uniform(0, 2*np.pi)  # Random angle\n",
    "n = 10  # Size of the feature vector\n",
    "expanded_feature_vector = expand_angle_to_vector(phi, n)\n",
    "print(\"Expanded Feature Vector:\", expanded_feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Multihead Self-Attention mechanism.\n",
    "\n",
    "    Attributes:\n",
    "        num_heads (int): Number of attention heads.\n",
    "        dim_q (int): Dimensionality of query vectors.\n",
    "        dim_k (int): Dimensionality of key vectors.\n",
    "        dim_v (int): Dimensionality of value vectors.\n",
    "        query (nn.Linear): Linear transformation for query vectors.\n",
    "        key (nn.Linear): Linear transformation for key vectors.\n",
    "        value (nn.Linear): Linear transformation for value vectors.\n",
    "        unifyheads (nn.Linear): Linear transformation to unify outputs from all heads.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimensionality of the input feature vector.\n",
    "        dim_q (int): Dimensionality of query vectors.\n",
    "        dim_k (int): Dimensionality of key vectors.\n",
    "        dim_v (int): Dimensionality of value vectors.\n",
    "        num_heads (int): Number of attention heads. Defaults to 8.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int, dim_v: int, num_heads: int = 8):\n",
    "        super(MultiheadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_q = dim_q\n",
    "        self.dim_k = dim_k\n",
    "        self.dim_v = dim_v\n",
    "\n",
    "        self.query = nn.Linear(dim_in, dim_q * num_heads)\n",
    "        self.key = nn.Linear(dim_in, dim_k * num_heads)\n",
    "        self.value = nn.Linear(dim_in, dim_v * num_heads)\n",
    "\n",
    "        self.unifyheads = nn.Linear(dim_v * num_heads, dim_in)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the MultiheadSelfAttention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, dim_in).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_length, dim_in) after applying multihead self-attention.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        queries = self.query(x).view(batch_size, seq_length, self.num_heads, self.dim_q)\n",
    "        keys = self.key(x).view(batch_size, seq_length, self.num_heads, self.dim_k)\n",
    "        values = self.value(x).view(batch_size, seq_length, self.num_heads, self.dim_v)\n",
    "\n",
    "        keys = keys.transpose(1, 2).contiguous().view(batch_size * self.num_heads, seq_length, self.dim_k)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(batch_size * self.num_heads, seq_length, self.dim_q)\n",
    "        values = values.transpose(1, 2).contiguous().view(batch_size * self.num_heads, seq_length, self.dim_v)\n",
    "\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.dim_k ** 0.5)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "\n",
    "        out = torch.bmm(attention, values).view(batch_size, self.num_heads, seq_length, self.dim_v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, self.dim_v * self.num_heads)\n",
    "\n",
    "        return self.unifyheads(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder block that applies a fully connected layer, ReLU activation, and another fully connected layer.\n",
    "\n",
    "    Attributes:\n",
    "        fc (nn.Sequential): A sequential container of layers.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): The dimensionality of the input features.\n",
    "        dim_out (int): The dimensionality of the output features.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in: int, dim_out: int):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_out, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the sequential container.\n",
    "        \"\"\"\n",
    "        return self.fc(x)\n",
    "\n",
    "class PoolingBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A pooling block that applies max pooling over the sequence dimension and a linear transformation.\n",
    "\n",
    "    Attributes:\n",
    "        pooling (nn.Linear): A linear layer for transforming the pooled output.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): The dimensionality of the input features.\n",
    "        num_outputs (int): The number of output features after pooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in: int, num_outputs: int):\n",
    "        super(PoolingBlock, self).__init__()\n",
    "        self.pooling = nn.Linear(dim_in, num_outputs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the PoolingBlock.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after max pooling and linear transformation.\n",
    "        \"\"\"\n",
    "        return self.pooling(x.max(dim=1).values)\n",
    "\n",
    "class SetTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Set Transformer model comprising an encoder, pooling, and decoder block.\n",
    "\n",
    "    Args:\n",
    "        encoder_block (nn.Module): The encoder block, typically a MultiheadSelfAttention module.\n",
    "        pooling_block (nn.Module): The pooling block.\n",
    "        decoder_block (nn.Module): The decoder block.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_block: nn.Module,\n",
    "        pooling_block: nn.Module,\n",
    "        decoder_block: nn.Module\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.encoder = encoder_block\n",
    "        self.pooling = pooling_block\n",
    "        self.decoder = decoder_block\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the SetTransformer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through encoder, pooling, and decoder blocks.\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Example usage and parameters are provided in the original script.\n",
    "\n",
    "# Parameters\n",
    "batch_size = 5\n",
    "seq_length = 10  # Number of coordinates\n",
    "input_dim = 2  # Dimension of each coordinate (Xfov, Yfov)\n",
    "dim_q = dim_k = dim_v = 64  # Dimension for query, key, value\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_outputs = 1  # Adjust based on your specific needs\n",
    "output_dim = 128  # Example output dimension\n",
    "\n",
    "# Generate a batch of random coordinates\n",
    "random_coordinates = torch.rand(batch_size, seq_length, input_dim)\n",
    "\n",
    "set_transformer = SetTransformer(\n",
    "    encoder_block=MultiheadSelfAttention(dim_in=input_dim, dim_q=dim_q, dim_k=dim_k, dim_v=dim_v, num_heads=num_heads),\n",
    "    pooling_block=PoolingBlock(dim_in=input_dim, num_outputs=num_outputs),\n",
    "    decoder_block=DecoderBlock(dim_in=num_outputs, dim_out=output_dim)\n",
    ")\n",
    "\n",
    "# Pass the random coordinates through the SetTransformer module\n",
    "output = set_transformer(random_coordinates)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, attention, projection, mask_projection, max_sequence_len, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes the decoder module.\n",
    "\n",
    "        Args:\n",
    "            attention (nn.Module): The multi-head self-attention module.\n",
    "            projection (nn.Module): The linear projection module to the output space.\n",
    "            mask_projection (nn.Module): The module to mask out the future tokens.\n",
    "            max_sequence_len (int): The maximum length of the output sequence.\n",
    "            output_dim (int): The dimensionality of the output space (e.g., 2 for 2D coordinates).\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Assuming dim_q = dim_k = dim_v for simplicity\n",
    "        self.attention = attention\n",
    "        # Linear layer to project from the attention output back to the original space\n",
    "        self.projection = projection\n",
    "        self.mask_projection = mask_projection\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder.\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): The embeddings tensor of shape (batch_size, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The reconstructed set of coordinates of shape (batch_size, max_sequence_len, output_dim).\n",
    "        \"\"\"\n",
    "        # Apply multihead self-attention\n",
    "        attention_output = self.attention(embeddings)\n",
    "\n",
    "        # Project the output of the attention mechanism back to the original input space\n",
    "        projected_output = self.projection(attention_output)\n",
    "\n",
    "        # Reshape to match the expected output shape\n",
    "        output = projected_output.view(-1, self.max_sequence_len, self.output_dim)\n",
    "\n",
    "        mask = self.mask_projection(attention_output)\n",
    "\n",
    "        return output, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 20\n",
    "\n",
    "encoder = SetTransformer(\n",
    "    encoder_block=MultiheadSelfAttention(dim_in=input_dim, dim_q=dim_q, dim_k=dim_k, dim_v=dim_v, num_heads=num_heads),\n",
    "    pooling_block=PoolingBlock(dim_in=input_dim, num_outputs=num_outputs),\n",
    "    decoder_block=DecoderBlock(dim_in=num_outputs, dim_out=output_dim)\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    attention=MultiheadSelfAttention(dim_in=output_dim, dim_q=dim_q, dim_k=dim_k, dim_v=dim_v, num_heads=num_heads),\n",
    "    projection=nn.Linear(output_dim, input_dim*max_sequence_len),\n",
    "    mask_projection=nn.Sequential(\n",
    "        nn.Linear(output_dim, max_sequence_len),\n",
    "        nn.Softmax(dim=-1),\n",
    "        nn.Flatten(1),\n",
    "        nn.Unflatten(-1, (max_sequence_len, 1))\n",
    "    ),\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    output_dim=input_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_, mask = decoder(output.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VectorToMatrixDeconv(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "\n",
    "        super(VectorToMatrixDeconv, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # Assuming the input vector is treated as a 1D \"image\" of shape (1, 1, input_len)\n",
    "        # The goal is to expand it to the target shape (m, n)\n",
    "        # This example uses a kernel size and stride that would need to be adjusted\n",
    "        # based on the specific relationship between input_len and output_shape\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels=1, out_channels=1,\n",
    "                                         kernel_size=(n_channels, 1), stride=(n_channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to unwrap the input vector into a matrix.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, 1, m, n).\n",
    "        \"\"\"\n",
    "        # Reshape input to match the expected shape for ConvTranspose2d: (N, C, H, W)\n",
    "        x = x.view(-1, 1, 1, x.shape[-1])  # Add dummy dimensions for H and W\n",
    "        \n",
    "        # Apply the deconvolution operation\n",
    "        x = self.deconv(x)\n",
    "        \n",
    "        # Optionally, remove the channel dimension if it's not needed in the output\n",
    "        x = x.squeeze(1)  # Resulting shape: (batch_size, m, n)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_len = 100  # Example input length\n",
    "n_channels = 10  # Desired output shape (m, n)\n",
    "\n",
    "model = VectorToMatrixDeconv(n_channels=n_channels)\n",
    "\n",
    "# Create a dummy input vector\n",
    "x = torch.randn(10, input_len)  # Batch size of 1\n",
    "\n",
    "# Unwrap the vector into a matrix\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "class MatrixToMatrixDeconv(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        \"\"\"\n",
    "        Initializes the deconvolution layer to transform a matrix of size (m, n) into a matrix of size (p, q).\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): The shape of the input matrix (m, n).\n",
    "            output_shape (tuple): The desired shape of the output matrix (p, q).\n",
    "        \"\"\"\n",
    "        super(MatrixToMatrixDeconv, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        m, n = input_shape\n",
    "        p, q = output_shape\n",
    "\n",
    "        # Calculate the kernel size, stride, and padding required to achieve the desired output shape.\n",
    "        # These values are placeholders and should be adjusted based on the specific requirements of your task.\n",
    "        kernel_size = (p // m, q // n)  # Example calculation\n",
    "        stride = (p // m, q // n)  # Example calculation\n",
    "        padding = (0, 0)  # Adjust as needed\n",
    "\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels=1, out_channels=1,\n",
    "                                         kernel_size=kernel_size, stride=stride,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to transform the input matrix into the desired output matrix shape.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 1, m, n).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, 1, p, q).\n",
    "        \"\"\"\n",
    "        # Apply the deconvolution operation\n",
    "        x = self.deconv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_shape = (10, 10)  # Shape of the input matrix (m, n)\n",
    "output_shape = (20, 20)  # Desired shape of the output matrix (p, q)\n",
    "\n",
    "model = MatrixToMatrixDeconv(input_shape=input_shape, output_shape=output_shape)\n",
    "\n",
    "# Create a dummy input matrix\n",
    "x = torch.randn(1, *input_shape)  # Batch size of 1, 1 channel\n",
    "\n",
    "# Transform the matrix\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralPopulationDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, m, n, p, q, hidden_dims = None):\n",
    "        super(NeuralPopulationDecoder, self).__init__()\n",
    "        if hidden_dims is None:\n",
    "            self.hidden_dims = (512, 256)\n",
    "        else:\n",
    "            self.hidden_dims = hidden_dims\n",
    "        self.m, self.n, self.p, self.q = m, n, p, q\n",
    "        self.fc1 = nn.Linear(input_dim, self.hidden_dims[0])  # Example dimension\n",
    "        self.fc2 = nn.Linear(self.hidden_dims[0], self.hidden_dims[1])  # Further processing\n",
    "\n",
    "        # Output layers for each population\n",
    "        self.fc_pop1_4 = nn.Linear(self.hidden_dims[1], self.hidden_dims[1])\n",
    "        self.pop1_4 = VectorToMatrixDeconv(4)\n",
    "        self.pop5 = nn.Linear(self.hidden_dims[1], n)  # Population 5\n",
    "        # self.pop6 = nn.Linear(self.hidden_dims[1], p*q)  # Population 6\n",
    "        self.pop6 = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims[1], (p*q)//8), # ((p*q)//8,)\n",
    "            VectorToMatrixDeconv((p*q)//8), # ((p*q)//8, (p*q)//8)\n",
    "            nn.BatchNorm2d((p*q)//8),\n",
    "            nn.ReLU(),\n",
    "            MatrixToMatrixDeconv(((p*q)//8, (p*q)//8), (p//4, q//4)),\n",
    "            nn.BatchNorm2d(p//4),\n",
    "            nn.ReLU(),\n",
    "            MatrixToMatrixDeconv((p//4, q//4), (p//2, q//2)),\n",
    "            nn.BatchNorm2d(p//2),\n",
    "            nn.ReLU(),\n",
    "            MatrixToMatrixDeconv((p//2, q//2), (p, q)),\n",
    "            nn.BatchNorm2d(p),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        x = F.relu(self.fc1(embeddings))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Generate outputs for each population\n",
    "        outputs_1_4 = self.pop1_4(self.fc_pop1_4(x))\n",
    "        output_5 = F.softmax(self.pop5(x), dim=-1)  # Assuming n represents categories\n",
    "        output_6 = self.pop6(x).view(-1, self.p, self.q)  # Reshape for place cells\n",
    "\n",
    "        return outputs_1_4, output_5, output_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationEmbeddingsDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_filters,\n",
    "        filter_length,\n",
    "        pop5_dim,\n",
    "        pop6_dims,\n",
    "        embedding_dim\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the decoder module.\n",
    "\n",
    "        Args:\n",
    "            n_filters (int): Number of filters for the convolutional layers.\n",
    "            filter_length (int): Length of the 1D convolutional filters for populations 1-4.\n",
    "            pop5_dim (int): Dimensionality of population 5.\n",
    "            pop6_dims (tuple): Dimensions of population 6 matrix, expected to be a tuple (p, q).\n",
    "            embedding_dim (int): Desired dimensionality of the output embeddings.\n",
    "        \"\"\"\n",
    "        super(PopulationEmbeddingsDecoder, self).__init__()\n",
    "        p, q = pop6_dims  # Unpack dimensions for population 6\n",
    "\n",
    "        # Convolution for populations 1-4\n",
    "        self.conv1_4 = nn.Conv1d(in_channels=4, out_channels=n_filters, kernel_size=filter_length)\n",
    "        self.pool1_4 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        # Linear for population 5\n",
    "        self.linear5 = nn.Linear(pop5_dim, 128)  # Example dimension\n",
    "\n",
    "        # CNN for population 6\n",
    "        self.conv6 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)  # Assuming population 6 has a single channel\n",
    "        self.pool6 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc6 = nn.Linear(64 * (p//2) * (q//2), 128)  # Adjust based on the size of population 6 after pooling\n",
    "\n",
    "        # Calculate the flattened size after pooling for populations 1-4\n",
    "        self.flattened_size_1_4 = n_filters * ((filter_length - 1) // 2)  # Adjust based on convolution and pooling\n",
    "\n",
    "        # Final projection to embeddings\n",
    "        self.final_projection = nn.Linear(self.flattened_size_1_4 + 128 + 128, embedding_dim)\n",
    "\n",
    "    def forward(self, pop1_4, pop5, pop6):\n",
    "        # Process populations 1-4\n",
    "        pop1_4 = pop1_4.permute(0, 2, 1)  # Assuming shape (batch_size, 4, sequence_length)\n",
    "        x1_4 = self.pool1_4(F.relu(self.conv1_4(pop1_4)))\n",
    "        x1_4 = x1_4.view(x1_4.size(0), -1)  # Flatten\n",
    "\n",
    "        # Process population 5\n",
    "        x5 = F.relu(self.linear5(pop5))\n",
    "\n",
    "        # Process population 6\n",
    "        x6 = self.pool6(F.relu(self.conv6(pop6.unsqueeze(1))))  # Add channel dimension\n",
    "        x6 = x6.view(x6.size(0), -1)  # Flatten\n",
    "        x6 = F.relu(self.fc6(x6))\n",
    "\n",
    "        # Concatenate and project to embeddings\n",
    "        x = torch.cat([x1_4, x5, x6], dim=1)\n",
    "        embeddings = self.final_projection(x)\n",
    "\n",
    "        return embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
